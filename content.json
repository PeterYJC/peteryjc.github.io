{"pages":[],"posts":[{"title":"Java中static, final, static final的区别","text":"通常情况下， 类成员需要通过它的类的对象访问，如果一个成员被声明为static，它能够在它的类的任何对象创建之前被访问， 而不用引用任何对象。 声明为static的一个类变量或方法，所有的该类的实例都会共享这个static变量或方法。 staticstatic修饰变量 静态变量在内存中只有一份， jvm只为静态变量分配一次内存，随着类的加载而加载到静态方法区内存中。由于静态变量属于类，和类的实例无关， 所以可以直接通过类名进行访问。 对于成员变量，每创建一个该类的实例就会创建该成员变量的一个拷贝，分配一次内存，由于成员变量是和类的实例绑定的，所以不能直接通过类名对它进行访问。 static修饰方法 只能调用其他的static方法 只能访问static数据 不能以任何方式引用this 和super 静态方法可以直接通过类名调用， 任何该类的实例也可以调用它的静态方法， 所以静态方法不能用this或者super。 static 方法独立于任何实例， 所以static方法必须被实现，不能是抽象的absract，在static方法里引用任何的实例变量都是违法的。 static 修饰类普通类不允许被声明为静态， 只有内部类才可以。被static修饰的内部类可以作为一个普通类来使用， 而不需实例一个外部类（不需要new，直接静态加载）。 内部类没有使用static关键字，不能直接创建实例。 不使用static修饰内部类 12345678910111213public class OuterClass { public class InnerClass{ InnerClass(){} }}public class TestStaticClass { public static void main(String[] args) { // OutClass需要先生成一个实例 OuterClass oc = new OuterClass(); oc.new InnerClass(); }} 使用static修饰内部类 123456789101112public class OuterClass { public static class InnerClass{ InnerClass(){} }}public class TestStaticClass { public static void main(String[] args) { // OutClass 不需要生成实例 new OuterClass.InnerClass(); }} static修饰代码块123static { System.out.println(\"test\");} 它独立于类成员，可以有多个， jvm 加载类的时候会执行这些静态代码块， 如果有static代码块多个，jvm会按照他们在类中出现的顺序执行且每个只执行一次。可以通过静态代码块对static变量进行赋值。 finalfinal可以修饰非抽象类， 非抽象类成员方法和变量 final修饰变量一个变量可以声明为final， 目的是阻止它的内容被修改， 这意味着声明final变量的时候， 必须对其进行初始化，这种用法有点类似于c++的const。 通常，我们会用 final定义一些常量 ， 如 123final float PI=3.14final boolean SUCCESS = true..... 按照编码约定， final变量的所有字符选择大写，final修饰的变量实际中不占用内存， 它实质上是一个常数。 final修饰方法被final修饰的方法可以被子类继承， 但不能被子类的方法覆盖。 如果一个类不想让其子类覆盖它的某个成员方法， 就可以用 final关键字修饰该方法。 final不能修饰构造方法。 由于父类中private 成员方法不能被子类覆盖， 所有由private修饰的方法默认也是final的。 使用final修饰成员方法除了不想让子类覆盖外， 还有一个原因就是高效，Java编译器在遇到final修饰的方法的时候会转入内嵌机制， 提高执行效率。 内嵌机制 ，类似于c++ inline, 调用方法的时候直接将方法的主题插入到调用处， 而不用去访问类或者对象， 这样会提高50%左右效率。然而，如果方法主体比较庞大， 且多处被调用将导致主体代码膨胀， 同时也产生效率问题， 所以需要慎用。 final修饰类final修饰的类不能被继承。 final和static同时使用同时使用 final和static修饰类成员， 该类成员拥有二者特性。 1static final int LIMIT=100; // LIMIT表示全局常量 如果是方法的话，方法可以被继承， 可以通过类名被访问， 但是不能被子类覆盖。 对于一些用final和static修饰的容器类型（比如，ArrayList、HashMap）的实例变量，不可以改变容器变量本身，但可以修改容器中存放的对象。","link":"/2019/11/04/final-static-finalwithstatic/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2019/11/04/hello-world/"},{"title":"迁移一个Gitlab项目至Gerrit","text":"Gerrit采用的分支方法与标准Git不同。在Gerrit中，基本上每个分支都是受保护的分支，但是对分支的需求要低得多，因为Gerrit会根据需要自动创建它们。作为开发人员，我可以在master分支上工作，当我推送到origin时，Gerrit会自动在Origin中创建一个称为refs/for/master的“虚拟”分支，并将更改放入该分支。这类似于在问题中创建MR时GitLab合并请求创建分支的过程。区别在于，有refs/for/master是虚拟的，所以有很多-我有refs/for/master，有refs/for/master，每个推送的人在原始仓库中都有一个。更改将保留在该虚拟分支上，直到按照规则完成审核为止，这时它将合并，并且虚拟分支将被删除。Gerrit支持分支上的读写特权，而不仅仅是写特权。 目前也没有批量迁移gitlab项目至gerrit的工具，比较保险的方法如下： 1)使用UI在Gerrit中创建存储库(或要求Gerrit管理员执行此操作) 2)使用” –bare”选项克隆Gitlab存储库 1git clone --bare GITLAB-URL 3)添加Gerrit remote 12cd REPO-NAMEgit remote add gerrit GERRIT-URL 4)将所有提交，分支和标签推送到Gerrit 12git push --all gerritgit push --tags gerrit 5)删除bare repo 12cd ..rm -rf REPO-NAME 以上请确保已经添加了公钥到新的机器上，以下是clone时的区别： git clone origin-url（非裸）：您将得到所有复制的标签，一个本地分支master (HEAD)追踪远程分支origin/master和远程分支origin/next，origin/pu和origin/maint。设置了跟踪分支，这样如果你做了类似的事情git fetch origin，它们就会像你期望的那样被提取。任何远程分支（在克隆的远程中）和其他引用都被完全忽略。 git clone –bare origin-url：您将获得全部复制的标签，地方分支机构master (HEAD)，next，pu，和maint，没有远程跟踪分支。也就是说，所有分支都按原样复制，并且它设置为完全独立，不期望再次获取。任何远程分支（在克隆的远程中）和其他引用都被完全忽略。 git clone –mirror origin-url：这些引用中的每一个都将按原样复制。你会得到所有的标签，地方分支机构master (HEAD)，next，pu，和maint，远程分支机构devA/master和devB/master其他裁判refs/foo/bar和refs/foo/baz。一切都与克隆的遥控器完全一样。设置远程跟踪，以便在运行时，git remote update所有引用都将从原点覆盖，就像您刚删除镜像并重新克隆它一样。正如文档最初所说，它是一面镜子。它应该是功能相同的副本，可与原始版本互换。 经测试，clone选项 –bare和–mirror均可成功实现迁移，不过还是建议用–bare克隆裸库。","link":"/2020/02/27/moveagitlabrepotogerrit/"},{"title":"为什么说Python是伪多线程","text":"学过操作系统的同学都知道，线程是现代操作系统底层一种轻量级的多任务机制。一个进程空间中可以存在多个线程，每个线程代表一条控制流，共享全局进程空间的变量，又有自己私有的内存空间。多个线程可以同时执行。此处的“同时”，在较早的单核架构中表现为“伪并行”，即让线程以极短的时间间隔交替执行，从人的感觉上看它们就像在同时执行一样。但由于仅有一个运算单元，当线程皆执行计算密集型任务时，多线程可能会出现 1 + 1 &gt; 2 的反效果。 GIL 是什么GIL 的全名是 the Global Interpreter Lock （全局解释锁），是常规 python 解释器（当然，有些解释器没有）的核心部件。我们看看官方的解释： The Python interpreter is not fully thread-safe. In order to support multi-threaded Python programs, there’s a global lock, called the global interpreter lock or GIL, that must be held by the current thread before it can safely access Python objects. 这是一个用于保护 Python 内部对象的全局锁（在进程空间中唯一），保障了解释器的线程安全。 这里用一个形象的例子来说明 GIL 的必要性（对资源抢占问题非常熟悉的可以跳过不看）： 我们把整个进程空间看做一个车间，把线程看成是多条不相交的流水线，把线程控制流中的字节码看作是流水线上待处理的物品。Python 解释器是工人，整个车间仅此一名。操作系统是一只上帝之手，会随时把工人从一条流水线调到另一条——这种“随时”是不由分说的，即不管处理完当前物品与否。若没有 GIL。假设工人正在流水线 A 处理 A1 物品，根据 A1 的需要将房间温度（一个全局对象）调到了 20 度。这时上帝之手发动了，工人被调到流水线 B 处理 B1 物品，根据 B1 的需要又将房间温度调到了 50 度。这时上帝之手又发动了，工人又调回 A 继续处理 A1。但此时 A1 暴露在了 50 度的环境中，安全问题就此产生了。而 GIL 相当于一条锁链，一旦工人开始处理某条流水线上的物品，GIL 便会将工人和该流水线锁在一起。而被锁住的工人只会处理该流水线上的物品。就算突然被调到另一条流水线，他也不会干活，而是干等至重新调回原来的流水线。这样每个物品在被处理的过程中便总是能保证全局环境不会突变。 该怎么提升效率呢GIL 是 Python 解释器正确运行的保证，Python 语言本身没有提供任何机制访问它。但在特定场合，我们仍有办法降低它对效率的影响。 使用多进程线程间会竞争资源是因为它们共享同一个进程空间，但进程的内存空间是独立的，自然也就没有必要使用解释锁了。 许多人非常忌讳使用多进程，理由是进程操作（创建、切换）的时间开销太大了，而且会占用更多的内存。这种担心其实没有必要——除非是对并发量要求很高的应用（如服务器），多进程增加的时空开销其实都在可以接受的范围中。更何况，我们可以使用进程池减少频繁创建进程带来的开销。 使用C拓展GIL 并不是完全的黑箱，CPython 在解释器层提供了控制 GIL 的开关——这就是 Py_BEGIN_ALLOW_THREADS 和 Py_END_ALLOW_THREADS 宏。这一对宏允许你在自定义的 C 扩展中释放 GIL，从而可以重新利用多核的优势。 沿用上面的例子，自定义的 C 扩展函数好比是流水线上一个特殊的物品。这个物品承诺自己不依赖全局环境，同时也不会要求工人去改变全局环境。同时它带有 Py_BEGIN_ALLOW_THREADS 和 Py_END_ALLOW_THREADS 两个机关，前者能砍断 GIL 锁链，这样工人被调度走后不需要干等，而是可以直接干活；后者则将锁链重新锁上，保证操作的一致性。 面试的高频点有同学可能会奇怪了：我在使用 python 多线程写爬虫时可从来没有这种问题啊——用 4 个线程下载 4 个页面的时间与单线程下载一个页面的时间相差无几。 这里就要谈到 GIL 的第二种释放时机了。除了调用 Py_BEGIN_ALLOW_THREADS，解释器还会在发生阻塞 IO（如网络、文件）时释放 GIL。发生阻塞 IO 时，调用方线程会被挂起，无法进行任何操作，直至内核返回；IO 函数一般是原子性的，这确保了调用的线程安全性。因此在大多数阻塞 IO 发生时，解释器没有理由加锁。 以爬虫为例：当 Thread1 发起对 Page1 的请求后，Thread1 会被挂起，此时 GIL 释放。当控制流切换至 Thread2 时，由于没有 GIL，不必干等，而是可以直接请求 Page2……如此一来，四个请求可以认为是几乎同时发起的。时间开销便与单线程请求一次一样。 有人反对使用阻塞IO，因为若想更好利用阻塞时的时间，必须使用多线程或进程，这样会有很大的上下文切换开销，而非阻塞 IO + 协程显然是更经济的方式。但当若干任务之间没有偏序关系时，一个任务阻塞是可以接受的（毕竟不会影响到其他任务的执行），同时也会简化程序的设计。而在一些通信模型（如 Publisher-Subscriber）中，“阻塞”是必要的语义。 多个阻塞IO需要多条非抢占式的控制流来承载，这些工作交给线程再合适不过了。","link":"/2019/11/22/python-GIL/"},{"title":"Docker与LXC简介","text":"Docker 是一个开源的应用容器引擎，基于 Go 语言 并遵从Apache2.0协议开源。Docker 可以让开发者打包他们的应用以及依赖包到一个轻量级、可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。容器是完全使用沙箱机制，相互之间不会有任何接口（类似 iPhone 的 app）,更重要的是容器性能开销极低。 Linux Container容器是一种内核虚拟化技术，可以提供轻量级的虚拟化，以便隔离进程和资源。 Docker的实现Docker 底层的核心技术包括 Linux 上的命名空间（Namespaces）、控制组（Control groups）、Union 文件系统（Union file systems）和容器格式（Container format）。 我们知道，传统的虚拟机通过在宿主主机中运行 hypervisor 来模拟一整套完整的硬件环境提供给虚拟机的操作系统。虚拟机系统看到的环境是可限制的，也是彼此隔离的。 这种直接的做法实现了对资源最完整的封装，但很多时候往往意味着系统资源的浪费。 例如，以宿主机和虚拟机系统都为 Linux 系统为例，虚拟机中运行的应用其实可以利用宿主机系统中的运行环境。 在操作系统中，包括内核、文件系统、网络、PID、UID、IPC、内存、硬盘、CPU 等等，所有的资源都是应用进程直接共享的。 要想实现虚拟化，除了要实现对内存、CPU、网络IO、硬盘IO、存储空间等的限制外，还要实现文件系统、网络、PID、UID、IPC等等的相互隔离。 前者相对容易实现一些，后者则需要宿主机系统的深入支持。 随着 Linux 系统对于命名空间功能的完善实现，程序员已经可以实现上面的所有需求，让某些进程在彼此隔离的命名空间中运行。大家虽然都共用一个内核和某些运行时环境（例如一些系统命令和系统库），但是彼此却看不到，都以为系统中只有自己的存在。这种机制就是容器（Container），利用命名空间来做权限的隔离控制，利用cgroups来做资源分配。 命名空间是 Linux 内核一个强大的特性。每个容器都有自己单独的命名空间，运行在其中的应用都像是在独立的操作系统中运行一样。命名空间保证了容器之间彼此互不影响。 pid 命名空间不同用户的进程就是通过 pid 命名空间隔离开的，且不同命名空间中可以有相同 pid。所有的 LXC 进程在 Docker 中的父进程为Docker进程，每个 LXC 进程具有不同的命名空间。同时由于允许嵌套，因此可以很方便的实现嵌套的 Docker 容器。 net 命名空间有了 pid 命名空间, 每个命名空间中的 pid 能够相互隔离，但是网络端口还是共享 host 的端口。网络隔离是通过 net 命名空间实现的， 每个 net 命名空间有独立的 网络设备, IP 地址, 路由表, /proc/net 目录。这样每个容器的网络就能隔离开来。Docker 默认采用 veth 的方式，将容器中的虚拟网卡同 host 上的一 个Docker 网桥 docker0 连接在一起。 ipc 命名空间容器中进程交互还是采用了 Linux 常见的进程间交互方法(interprocess communication - IPC), 包括信号量、消息队列和共享内存等。然而同 VM 不同的是，容器的进程间交互实际上还是 host 上具有相同 pid 命名空间中的进程间交互，因此需要在 IPC 资源申请时加入命名空间信息，每个 IPC 资源有一个唯一的 32 位 id。 mnt 命名空间类似 chroot，将一个进程放到一个特定的目录执行。mnt 命名空间允许不同命名空间的进程看到的文件结构不同，这样每个命名空间 中的进程所看到的文件目录就被隔离开了。同 chroot 不同，每个命名空间中的容器在 /proc/mounts 的信息只包含所在命名空间的 mount point。 uts 命名空间UTS(“UNIX Time-sharing System”) 命名空间允许每个容器拥有独立的 hostname 和 domain name, 使其在网络上可以被视作一个独立的节点而非 主机上的一个进程。 user 命名空间每个容器可以有不同的用户和组 id, 也就是说可以在容器内用容器内部的用户执行程序而非主机上的用户。 注：更多关于 Linux 上命名空间的信息，请阅读 这篇文章。 Linux Container简介LXC为Linux Container的简写。可以提供轻量级的虚拟化，以便隔离进程和资源，而且不需要提供指令解释机制以及全虚拟化的其他复杂性。相当于C 中的NameSpace。容器有效地将由单个操作系统管理的资源划分到孤立的组中，以更好地在孤立的组之间平衡有冲突的资源使用需求。与传统虚拟化技术相比，它的优势在于： （1）与宿主机使用同一个内核，性能损耗小； （2）不需要指令级模拟； （3）不需要即时(Just-in-time)编译； （4）容器可以在CPU核心的本地运行指令，不需要任何专门的解释机制； （5）避免了准虚拟化和系统调用替换中的复杂性； （6）轻量级隔离，在隔离的同时还提供共享机制，以实现容器与宿主机的资源共享。 换句话说，Linux Container是一种轻量级的虚拟化的手段。","link":"/2019/11/18/simple-docker/"},{"title":"用户态和内核态的区别","text":"由于需要限制不同的程序之间的访问能力, 防止他们获取别的程序的内存数据, 或者获取外围设备的数据, 并发送到网络, CPU划分出两个权限等级—用户态和内核态。 什么是用户态和内核态内核态：cpu可以访问内存的所有数据，包括外围设备，例如硬盘，网卡，cpu也可以将自己从一个程序切换到另一个程序。 用户态：只能受限的访问内存，且不允许访问外围设备，占用cpu的能力被剥夺，cpu资源可以被其他程序获取。 用户态与内核态的切换所有用户程序都是运行在用户态的, 但是有时候程序确实需要做一些内核态的事情, 例如从硬盘读取数据, 或者从键盘获取输入等. 而唯一可以做这些事情的就是操作系统, 所以此时程序就需要先操作系统请求以程序的名义来执行这些操作。 内核态与用户态是操作系统的两种运行级别,跟intel cpu没有必然的联系,intel cpu提供Ring0-Ring3三种级别的运行模式，Ring0级别最高，Ring3最低。Linux使用了Ring3级别运行用户态，Ring0作为内核态，没有使用Ring1和Ring2。Ring3状态不能访问Ring0的地址空间，包括代码和数据。Linux进程的4GB地址空间，3G-4G部 分大家是共享的，是内核态的地址空间，这里存放在整个内核的代码和所有的内核模块，以及内核所维护的数据。用户运行一个程序，该程序所创建的进程开始是运 行在用户态的，如果要执行文件操作，网络数据发送等操作，必须通过write，send等系统调用，这些系统调用会调用内核中的代码来完成操作，这时，必须切换到Ring0，然后进入3GB-4GB中的内核地址空间去执行这些代码完成操作，完成后，切换回Ring3，回到用户态。这样，用户态的程序就不能 随意操作内核地址空间，具有一定的安全保护作用。 这时需要一个这样的机制: 用户态程序切换到内核态, 但是不能控制在内核态中执行的指令。 这种机制叫系统调用, 在CPU中的实现称之为陷阱指令(Trap Instruction)当一个任务（进程）执行系统调用而陷入内核代码中执行时，我们就称进程处于内核运行态（或简称为内核态）。此时处理器处于特权级最高的（0级）内核代码中执行。当进程处于内核态时，执行的内核代码会使用当前进程的内核栈。每个进程都有自己的内核栈。当进程在执行用户自己的代码时，则称其处于用户运行态（用户态）。即此时处理器在特权级最低的（3级）用户代码中运行。当正在执行用户程序而突然被中断程序中断时，此时用户程序也可以象征性地称为处于进程的内核态。因为中断处理程序将使用当前进程的内核栈。这与处于内核态的进程的状态有些类似。用户态和内核态的转换 用户态切换到内核态的3种方式系统调用这是用户态进程主动要求切换到内核态的一种方式，用户态进程通过系统调用申请使用操作系统提供的服务程序完成工作，比如前例中fork()实际上就是执行了一个创建新进程的系统调用。而系统调用的机制其核心还是使用了操作系统为用户特别开放的一个中断来实现，例如Linux的int 80h中断。 异常当CPU在执行运行在用户态下的程序时，发生了某些事先不可知的异常，这时会触发由当前运行进程切换到处理此异常的内核相关程序中，也就转到了内核态，比如缺页异常。 外围设备的中断当外围设备完成用户请求的操作后，会向CPU发出相应的中断信号，这时CPU会暂停执行下一条即将要执行的指令转而去执行与中断信号对应的处理程序，如果先前执行的指令是用户态下的程序，那么这个转换的过程自然也就发生了由用户态到内核态的切换。比如硬盘读写操作完成，系统会切换到硬盘读写的中断处理程序中执行后续操作等。 这3种方式是系统在运行时由用户态转到内核态的最主要方式，其中系统调用可以认为是用户进程主动发起的，异常和外围设备中断则是被动的。 具体的切换操作从触发方式上看，可以认为存在前述3种不同的类型，但是从最终实际完成由用户态到内核态的切换操作上来说，涉及的关键步骤是完全一致的，没有任何区别，都相当于执行了一个中断响应的过程，因为系统调用实际上最终是中断机制实现的，而异常和中断的处理机制基本上也是一致的，关于它们的具体区别这里不再赘述。关于中断处理机制的细节和步骤这里也不做过多分析，涉及到由用户态切换到内核态的步骤主要包括： 123451. 用户态程序将一些数据值放在寄存器中, 或者使用参数创建一个堆栈(stack frame), 以此表明需要操作系统提供的服务。2. 用户态程序执行陷阱指令。3. CPU切换到内核态, 并跳到位于内存指定位置的指令, 这些指令是操作系统的一部分, 他们具有内存保护, 不可被用户态程序访问4. 这些指令称之为陷阱(trap)或者系统调用处理器(system call handler). 他们会读取程序放入内存的数据参数, 并执行程序请求的服务。5. 系统调用完成后, 操作系统会重置CPU为用户态并返回系统调用的结果。 至于说保护模式，是说通过内存页表操作等机制，保证进程间的地址空间不会互相冲突，一个进程的操作不会修改另一个进程的地址空间中的数据。","link":"/2019/11/15/yonghutai-neihetai/"},{"title":"打家劫舍","text":"总结一下LeetCode 打家劫舍的专题。嘻嘻 打家劫舍你是一个专业的小偷，计划偷窃沿街的房屋。每间房内都藏有一定的现金，影响你偷窃的唯一制约因素就是相邻的房屋装有相互连通的防盗系统，如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警。 给定一个代表每个房屋存放金额的非负整数数组，计算你在不触动警报装置的情况下，能够偷窃到的最高金额。 示例 1: 输入: [1,2,3,1]输出: 4解释: 偷窃 1 号房屋 (金额 = 1) ，然后偷窃 3 号房屋 (金额 = 3)。 偷窃到的最高金额 = 1 + 3 = 4 。 示例 2: 输入: [2,7,9,3,1]输出: 12解释: 偷窃 1 号房屋 (金额 = 2), 偷窃 3 号房屋 (金额 = 9)，接着偷窃 5 号房屋 (金额 = 1)。 偷窃到的最高金额 = 2 + 9 + 1 = 12 。 思路：动态规划，要不这家或者前一家+后一家 max(nums[i]+dp[i-2],dp[i-1] 12345678910class Solution: def rob(self, nums: List[int]) -&gt; int: if(len(nums)==0): return 0 if(len(nums)==1): return nums[0] dp = [] dp.append(nums[0]) dp.append(max(nums[0],nums[1])) for i in range(2,len(nums)): dp.append(max(nums[i]+dp[i-2],dp[i-1])) return dp[len(nums)-1] 打家劫舍 II你是一个专业的小偷，计划偷窃沿街的房屋，每间房内都藏有一定的现金。这个地方所有的房屋都围成一圈，这意味着第一个房屋和最后一个房屋是紧挨着的。同时，相邻的房屋装有相互连通的防盗系统，如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警。 给定一个代表每个房屋存放金额的非负整数数组，计算你在不触动警报装置的情况下，能够偷窃到的最高金额。 示例 1: 输入: [2,3,2]输出: 3解释: 你不能先偷窃 1 号房屋（金额 = 2），然后偷窃 3 号房屋（金额 = 2）, 因为他们是相邻的。示例 2: 输入: [1,2,3,1]输出: 4解释: 你可以先偷窃 1 号房屋（金额 = 1），然后偷窃 3 号房屋（金额 = 3）。 偷窃到的最高金额 = 1 + 3 = 4 。 思路：和第一道题一样，不过分两种情况讨论，去掉第一家或者去掉最后一家，最后取大。 在不偷窃第一个房子的情况下（即 nums[1:]nums[1:]），最大金额是 p1 在不偷窃最后一个房子的情况下（即 nums[:n-1]nums[:n−1]），最大金额是 p2 综合偷窃最大金额： 为以上两种情况的较大值，即 max(p1,p2) 12345678class Solution: def rob(self, nums: [int]) -&gt; int: def my_rob(nums): cur, pre = 0, 0 for num in nums: cur, pre = max(pre + num, cur), cur return cur return max(my_rob(nums[:-1]),my_rob(nums[1:])) if len(nums) != 1 else nums[0] 打家劫舍 III在上次打劫完一条街道之后和一圈房屋后，小偷又发现了一个新的可行窃的地区。这个地区只有一个入口，我们称之为“根”。 除了“根”之外，每栋房子有且只有一个“父“房子与之相连。一番侦察之后，聪明的小偷意识到“这个地方的所有房屋的排列类似于一棵二叉树”。 如果两个直接相连的房子在同一天晚上被打劫，房屋将自动报警。 计算在不触动警报的情况下，小偷一晚能够盗取的最高金额。 示例 1: 输入: [3,2,3,null,3,null,1] 3 / \\ 2 3 \\ \\ 3 1输出: 7解释: 小偷一晚能够盗取的最高金额 = 3 + 3 + 1 = 7.示例 2: 输入: [3,4,5,1,3,null,1] 3 / \\ 4 5 / \\ \\ 1 3 1 输出: 9解释: 小偷一晚能够盗取的最高金额 = 4 + 5 = 9 思路： 我们可以先不管这个否是二叉树,我们发现,如果我们选了 cur 这个节点 那么就说明 我们不能选它的所有子节点(还有父节点)。对于每一个节点，都只有选和不选两种情况。我们每次考虑一棵子树，那么根只有两种情况，选和不选(我们让dp[0]表示不选,dp[1]表示选)。对于选择了根,那么我们就不能选它的儿子了如果没有选根，我们就可以任意选了(即选最大的那一个) 12345678910111213141516171819202122# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def dp(self , cur : TreeNode) -&gt; List[int] : if not cur : return [0,0] l = self.dp(cur.left) r = self.dp(cur.right) return [max(l)+max(r),cur.val+l[0]+r[0]] def rob(self, root: TreeNode) -&gt; int: return max(self.dp(root))","link":"/2019/12/12/打家劫舍/"},{"title":"Gitlab跨版本升级","text":"最近leader让调研一下gitlab升级方案，测试了两次，都成功了，暂时没有发现网上有比较完整的升级博客或是文档，所以我总结了一下，后续有时间再补充一下升级过程中遇到的问题。 推荐升级路径：Gitlab-ce无法保证各大版本之间无缝升级，所以推荐升级路径： 公司的升级顺序按理应为： 123456789101112131415161718199.4.3 -&gt; 9.5.10 -&gt; 10.8.7 -&gt;^ ^ 11.11.8 -&gt;| | ^ ^ 12.5.5| | | | ^| | | | || | | | || | | | +| | | | Target version| | | | | | | | | | | +| | | Last release in the 11.x major version series| | +| | Last release in the 10.x major version series| +| Last release in the 9.x major version series+Current release to upgrade 可以看到我们已经下载好了到12.5.5的安装包： 帮助升级文档 备份文件：(可选) 1234567# rsync同步 保证目录权限rsync -av /var/opt/gitlab/backups /data/git/backups# 配置文件中添加如下配置 覆盖默认配置vim /etc/gitlab/gitlab.rb# gitlab_rails[&apos;backup_path&apos;] = &quot;/var/opt/gitlab/backups&quot; gitlab_rails[&apos;backup_path&apos;] = &quot;/data/git/backups&quot; Gitlab版本说明Gitlab采用 (Major).(Minor).(Patch)的命名方式： 例如，对于GitLab版本10.5.7： 10代表主要版本。主要版本是10.0.0，但通常称为10.0。 5代表次要版本。次要版本是10.5.0，但通常称为10.5。 7 代表补丁号。 特定版本的更改更新到主要版本可能需要一些手动干预。有关详细信息，请检查要更新的版本： GitLab 12 GitLab 11 GitLab 11具体的变化TLS v1.1弃用从GitLab 12.0开始，默认情况下将禁用TLS v1.1以提高安全性。 这减轻了许多问题，包括但不限于Heartbleed，并使GitLab开箱即用，符合PCI DSS 3.1标准。 详细了解我们的博客中不推荐使用TLS v1.1的原因。 客户支持TLS v1.2 Git-Credential-Manager - 自1.14.0以来的支持 红帽企业Linux 6上的git - 自6.8以来的支持 Red Hat Enteprirse Linux 7上的git - 自7.2以来的支持 JGit / Java - 自JDK 7以来的支持 Visual Studio - 自2017年版以来的支持 修改或添加gitlab.rb并gitlab-ctl reconfigure立即运行以禁用TLS v1.1： 1nginx[&apos;ssl_protocols&apos;] = &quot;TLSv1.2&quot; 升级先决条件要成功升级到GitLab 11.0，用户需要满足以下要求： 用户应该在10.x系列中运行最新版本。即10.8.7 现在已删除10.x系列中已弃用的配置（下面列出）。需要将其删除/etc/gitlab/gitlab.rb。然后运行gitlab-ctl reconfigure以应用配置更改。 如果不满足上述任一要求，升级过程将中止而不更改用户的现有安装。这是为了确保用户不会因为这些不受支持的配置而导致Gitlab损坏。 删除配置以下配置在10.x系列中已弃用，现已删除： 与Mattermost相关的配置 - 除了GitLab-Mattermost集成所需的基本配置外，已删除对大多数Mattermost相关配置的支持。查看官方文档了解详细信息 旧版git_data_dir配置，用于设置数据存储位置。它现在已经被git_data_dirs 配置所取代。查看官方文档了解详细信息 旧格式的git_data_dirs配置已被替换为新格式，允许更精细的颗粒控制。查看官方文档了解详细信息 次要版本中引入的更改11.2默认情况下禁用机架攻击。要继续使用Rack Attack，您必须手动启用它。 11.4 捆绑Redis的版本已升级到3.2.12。这是一个修复多个漏洞的关键安全更新。升级到11.4后，运行gitlab-ctl restart redis以确保加载新版本。 Prometheus的捆绑版本 已升级到2.4.2，默认情况下新安装将使用它。Prometheus版本2使用与版本1不兼容的数据格式。 对于希望保留Prometheus版本1数据的用户，提供了一个命令行工具来升级其Prometheus服务并将数据迁移到新Prometheus版本支持的格式。可以使用以下命令调用此工具： 1sudo gitlab-ctl prometheus-upgrade 此工具将现有数据转换为最新Prometheus版本支持的格式。根据数据量，此过程可能需要数小时。如果用户不想迁移数据，但是从干净的数据库开始，则可以将--skip-data-migrationflag 传递给上面的命令。 注意：Prometheus服务将在迁移过程中停止。 要了解其他支持的选项，--help请将flag 传递给上面的命令。 程序包升级期间不会自动调用此工具。用户必须手动运行才能迁移到最新版本的Prometheus，并建议尽快进行。因此，升级到11.4的现有用户将继续使用Prometheus 1.x，直到他们手动迁移到2.x版本。 对早期版本的GitLab附带的Prometheus 1.x版本的支持已被弃用，将在GitLab 12.0中完全删除。仍在使用这些版本的用户将在重新配置期间显示弃用警告。使用GitLab 12.0 Prometheus将自动升级到2.x，Prometheus 1.0数据将无法迁移。 11.6 如果在Redis HA模式下配置GitLab，默认情况下将禁用gitlab-monitor的Sidekiq探针。要手动启用它，用户可以gitlab_monitor['probe_sidekiq'] = true在/etc/gitlab/gitlab.rb文件中进行设置。但是，在Redis HA模式下手动启用时，用户应使用gitlab_rails['redis_*']设置将探针指向连接到实例的Redis实例。 有效的示例配置是： 1234gitlab_monitor[&apos;probe_sidekiq&apos;] = truegitlab_rails[&apos;redis_host&apos;] = &lt;IP of Redis master node&gt;gitlab_rails[&apos;redis_port&apos;] = &lt;Port where Redis runs in master node&gt;gitlab_rails[&apos;redis_password&apos;] = &lt;Password to connect to Redis master&gt; 注意：在上面的配置中，当主节点发生故障后发生故障转移时，gitlab-monitor仍将探测原始主节点，因为它是在中指定的gitlab.rb。用户必须手动更新gitlab.rb以将其指向新的主节点。 Ruby已更新至2.5.3。GitLab将在升级期间关闭，直到重新启动独角兽进程。重启在结束时自动完成gitlab-ctl reconfigure，默认情况下在升级时运行。 GitLab 10 GitLab 8 GitLab 7 GitLab 6 升级文档 操作系统 Ubuntu 16.04.1 LTS 系统配置 CPU：8核 内存：8G 磁盘：2T（/var/opt/gitlab） 当前版本 GitLab：9.4.3 GitLab Shell：5.3.1 GitLab Workhorse：v2.3.0 GitLab API：v4 Git：2.13.0 Ruby：2.3.3p222 Rails：4.2.8 postgresql：9.6.3 目标版本 GitLab：12.5.5 GitLab Shell：10.2.0 GitLab Workhorse：v8.14.1 GitLab API：v4 Git：2.7.4 Ruby：2.6.3p62 Rails：5.2.3 postgresql：10.9 升级步骤获取安装包 gitlab-ce_9.5.10-ce.0_amd64.deb gitlab-ce_10.8.7-ce.0_amd64.deb gitlab-ce_11.11.8-ce.0_amd64.deb gitlab-ce_12.5.5-ce.0_amd64.deb 说明：由于直接从9.xx升级到12.xx，中间数据库结构变化很大，9.4.3- &gt;9.5.10- &gt;10.8.7- &gt;11.11.8-&gt;12.5.5 目前版本： 停止对外提供服务，避免数据不一致。(每次升级都必须) 123gitlab-ctl stop unicorngitlab-ctl stop sidekiqgitlab-ctl stop nginx 进行gitlab全备份或rsync（见上） su do gitlab-rake gitlab:backup:create 升级到9.5.10版本 sudo dpkg -i gitlab-ce_9.5.10-ce.0_amd64.deb 重启一下：sudo gitlab-ctl restart 登录gitlab页面检查（该版本已经支持healthcheck接口，通过healthcheck接口检查服务是否正常） ![image-20191218174321793](/Users/yangjiacheng/Library/Application Support/typora-user-images/image-20191218174321793.png) 升级到10.8.7版本 sudo dpkg -i gitlab-ce_10.8.7-ce.0_amd64.deb 重启一下：sudo gitlab-ctl restart 登录gitlab页面检查（通过healthcheck接口检查服务是否正常） 升级到11.11.8版本 sudo dpkg -i gitlab-ce_10.8.7-ce.0_amd64.deb 重启一下：sudo gitlab-ctl restart 登录gitlab页面检查（通过healthcheck接口检查服务是否正常） 升级到12.5.5版本 sudo dpkg -i gitlab-ce_12.5.5-ce.0_amd64.deb 重启一下：sudo gitlab-ctl restart 登录gitlab页面检查（通过healthcheck接口检查服务是否正常） 至此gitlab升级完毕 以上过程已经在测试环境完成测试 回滚方案： 将备份的文件在备份机器上进行恢复 gitlab-rake gitlab:backup:restore force=yes BACKUP=版本号 一旦升级失败，可以将域名指向备份机器","link":"/2019/12/18/Gitlab跨版本升级/"},{"title":"K8s小练习","text":"最近在知乎上发现了一个专栏，云计算技术前线,每天有一道题，对于检测基础来说很不错，由此我把它总结到这里。 第一题|Daemonset知识知识点初探以下 Daemonset yaml 中，哪些是正确的？（多选） A. apiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd-elasticsearch namespace: default labels: k8s-app: fluentd-logging spec: selector: matchLabels: name: fluentd-elasticsearch template: metadata: labels: name: fluentd-elasticsearch spec: containers: - name: fluentd-elasticsearch image: gcr.io/fluentd-elasticsearch/fluentd:v2.5.1 restartPolicy: Never B. apiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd-elasticsearch namespace: default labels: k8s-app: fluentd-logging spec: selector: matchLabels: name: fluentd-elasticsearch template: metadata: labels: name: fluentd-elasticsearch spec: containers: - name: fluentd-elasticsearch image: gcr.io/fluentd-elasticsearch/fluentd:v2.5.1 restartPolicy: Onfailure C. apiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd-elasticsearch namespace: default labels: k8s-app: fluentd-logging spec: selector: matchLabels: name: fluentd-elasticsearch template: metadata: labels: name: fluentd-elasticsearch spec: containers: - name: fluentd-elasticsearch image: gcr.io/fluentd-elasticsearch/fluentd:v2.5.1 restartPolicy: Always D. apiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd-elasticsearch namespace: default labels: k8s-app: fluentd-logging spec: selector: matchLabels: name: fluentd-elasticsearch template: metadata: labels: name: fluentd-elasticsearch spec: containers: - name: fluentd-elasticsearch image: gcr.io/fluentd-elasticsearch/fluentd:v2.5.1 解析：CD restartPolicy 字段，可选值为 Always、OnFailure 和 Never。默认为 Always。 一个Pod中可以有多个容器，restartPolicy适用于Pod 中的所有容器。restartPolicy作用是，让kubelet重启失败的容器。 另外Deployment、Statefulset的restartPolicy也必须为Always，保证pod异常退出，或者健康检查livenessProbe失败后由kubelet重启容器。https://kubernetes.io/zh/docs/concepts/workloads/controllers/deployment/ Job和CronJob是运行一次的pod，restartPolicy只能为OnFailure或Never，确保容器执行完成后不再重启。https://kubernetes.io/docs/conc 第二题| Daemonset、对接存储CSI知识点在Kubernetes PVC+PV体系下通过CSI实现的volume plugins动态创建pv到pv可被pod使用有哪些组件需要参与？ 123A. PersistentVolumeController + CSI-Provisoner + CSI controller pluginB. AttachDetachController + CSI-Attacher + CSI controller pluginC. Kubelet + CSI node plugin 解析：ABC k8s中，利用PVC 描述Pod 所希望使用的持久化存储的大小，可读写权限等，一般由开发人员去创建；利用PV描述具体存储类型，存储地址，挂载目录等，一般由运维人员去提前创建。而不是直接在pod里写上volume的信息。一来可以使得开发运维职责分明，二来利用PVC、PV机制，可以很好扩展支持市面上不同的存储实现，如k8s v1.10版本对Local Persistent Volume的支持。 我们试着理一下Pod创建到volume可用的整体流程。 用户提交请求创建pod，PersistentVolumeController发现这个pod声明使用了PVC，那就会帮它找一个PV配对。 没有现成的PV，就去找对应的StorageClass，帮它新创建一个PV，然后和PVC完成绑定。 新创建的PV，还只是一个API 对象，需要经过“两阶段处理”，才能变成宿主机上的“持久化 Volume”真正被使用： 第一阶段由运行在master上的AttachDetachController负责，为这个PV完成 Attach 操作，为宿主机挂载远程磁盘； 第二阶段是运行在每个节点上kubelet组件的内部，把第一步attach的远程磁盘 mount 到宿主机目录。这个控制循环叫VolumeManagerReconciler，运行在独立的Goroutine，不会阻塞kubelet主控制循环。 完成这两步，PV对应的“持久化 Volume”就准备好了，POD可以正常启动，将“持久化 Volume”挂载在容器内指定的路径。 k8s支持编写自己的存储插件FlexVolume 与 CSI。不管哪种方式，都需要经过“两阶段处理”，FlexVolume相比CSI局限性大，一般我们采用CSI方式对接存储。 CSI 插件体系的设计思想把这个Provision阶段（动态创建PV），以及 Kubernetes 里的一部分存储管理功能，从主干代码里剥离出来，做成了几个单独的组件。这些组件会通过 Watch API 监听 Kubernetes 里与存储相关的事件变化，比如 PVC 的创建，来执行具体的存储管理动作。 上图中CSI这套存储插件体系中三个独立的外部组件（External Components），即：Driver Registrar、External Provisioner 和 External Attacher，对应的是从 Kubernetes 项目里面剥离出来的部分存储管理功能。 我们需要实现Custom Components这一个二进制，会以gRpc方式提供三个服务：CSI Identity、CSI Controller、CSI Node。 Driver Registrar 组件，负责将插件注册到 kubelet 里面；Driver Registrar调用CSI Identity 服务来获取插件信息；External Provisioner 组件监听APIServer 里的 PVC 对象。当一个 PVC 被创建时，它就会调用 CSI Controller 的 CreateVolume 方法，创建对应 PV； External Attacher 组件负责Attach阶段。Mount阶段由kubelet里的VolumeManagerReconciler控制循环直接调用CSI Node服务完成。 两阶段完成后，kubelet将mount参数传递给docker，创建、启动容器。 整体流程如下图： 第三题|对接CSI存储知识通过单个命令创建一个deployment并暴露Service。deployment和Service名称为cka-1120，使用nginx镜像， deployment拥有2个pod 12345678910[root@peter ~]# kubectl run cka-1120 --replicas 2 --expose=true --port=80 --image=nginxkubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.service/cka-1120 createddeployment.apps/cka-1120 created[root@peter ~]# kubectl get all | grep cka-1120pod/cka-1120-554b9c4798-7jcrb 1/1 Running 0 118mpod/cka-1120-554b9c4798-fpjwj 1/1 Running 0 118mservice/cka-1120 ClusterIP 10.108.140.25 &lt;none&gt; 80/TCP 118mdeployment.apps/cka-1120 2/2 2 2 118m 解析： 官网中提供了详细的kubectl使用方法，位于REFERENCE–&gt;&gt;kubectl CLI–&gt;&gt;kubectl Commands标签下。即：https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#runkubectl run会创建deployment或者job来管理Pod，命令语法如下： 1kubectl run NAME --image=image [--env=&quot;key=value&quot;] [--port=port] [--replicas=replicas] [--dry-run=bool] [--overrides=inline-json] [--command] -- [COMMAND] [args...] NAME指定deployment和service的名称；–replicas缩写-r，指定实例数，默认为1；–expose如果为true，会创建有ClusterIP的service，默认为false；–port表示容器暴露的端口，如果expose为true，该端口也是service的端口；–image指定容器用的镜像；–dry-run为true时，只打印将要发送的对象，而不真正发送它，默认为false。 创建名为cka-1120-01，带环境变量的deployment 1kubectl run cka-1120-01 --image=nginx --env=&quot;DNS_DOMAIN=cluster.local&quot; --env=&quot;POD_NAMESPACE=default&quot; 创建名为cka-1120-02，带label的deployment 1kubectl run cka-1120-02 --image=nginx --labels=&quot;app=nginx,env=prod&quot; 还有一个–restart参数，默认为Always，如果设置为OnFailure，则job会被创建；如果设置为Never，则普通Pod会被创建。 123456[root@peter ~]# kubectl run cka-1120-03 --image=nginx --restart=OnFailurekubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.job.batch/cka-1120-03 created[root@peter ~]# [root@peter ~]# kubectl run cka-1120-04 --image=nginx --restart=Neverpod/cka-1120-04 created 参数–schedule指定cronjob的定时规则，如果指定该参数，则会创建出cronjob 12[root@peter ~]# kubectl run pi --schedule=&quot;0/5 * * * ?&quot; --image=perl --restart=OnFailure -- perl -Mbignum=bpi -wle &apos;print bpi(2000)&apos;cronjob.batch/pi created 目前不支持直接创建Statefulset、Daemonset等资源对象 kubectl run执行后，到底发生了什么？有必要看看kubectl源码，入口函数在$GOPATHsrck8s.iokubernetescmdclicheckcheckcliconventions.go中 其中cmd.NewKubectlCommand为构建kubectl以及其子命令行参数。最终的执行业务逻辑的代码都在pkgkubectl包下面。不同的子命令：apply、run、create入口对应的在pkgkubectlcmd下面： 最重要的o.Run(f, cmd, args)中会对kubectl run传入的参数进行一系列校验，填充默认值。 在360行调用o.createGeneratedObject根据不同的generator生成deployment、cronjob、job、pod等资源对象，并向apiserver发送创建请求。 如果设置了expose为true，在372行，同样的调用o.createGeneratedObject生成并创建service。 o.createGeneratedObject方法第649行，根据不同的generator实现生成不同的资源对象。 run命令对应的generator实现有以下几种，代码位于pkgkubectlgenerateversionedgenerator.go中的DefaultGenerators函数。 1234567891011case &quot;run&quot;: generator = map[string]generate.Generator{ RunV1GeneratorName: BasicReplicationController{}, RunPodV1GeneratorName: BasicPod{}, DeploymentV1Beta1GeneratorName: DeploymentV1Beta1{}, DeploymentAppsV1Beta1GeneratorName: DeploymentAppsV1Beta1{}, DeploymentAppsV1GeneratorName: DeploymentAppsV1{}, JobV1GeneratorName: JobV1{}, CronJobV2Alpha1GeneratorName: CronJobV2Alpha1{}, CronJobV1Beta1GeneratorName: CronJobV1Beta1{}, } o.createGeneratedObject方法第689行对生成的资源对象向APIServer发送http创建请求。 具体的kubectl run命令的代码，感兴趣的同学可以进一步深挖，我也会在后续的源码分析系列文章中进行更详细的解析。 第四题|熟练掌握kubectl命令进行创建资源对象操作通过命令行，使用nginx镜像创建一个pod并手动调度到节点名为node1121节点上，Pod的名称为cka-1121，答题最好附上，所用命令、创建Pod所需最精简的yaml；如果评论有限制，请把注意点列出，主要需列出手动调度怎么做？ 注意：手动调度是指不需要经过kube-scheduler去调度。 123456789101112apiVersion: v1kind: Podmetadata: name: cka-1121 labels: app: cka-1121spec: containers: - name: cka-1121 image: busybox command: ['sh', '-c', 'echo Hello CKA! &amp;&amp; sleep 3600'] nodeName: node1121 解析： 官网中调度器地址：https://kubernetes.io/docs/concepts/scheduling/kube-scheduler/调度器命令行参数：https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/ 调度器kube-scheduler分为预选、优选、pod优先级抢占、bind阶段； 预选：从podQueue的待调度队列中弹出需要调度的pod，先进入预选阶段，预选函数来判断每个节点是否适合被该Pod调度。 优选：从预选筛选出的满足的节点中选择出最优的节点。 pod优先级抢占：如果预选和优选调度失败，则会尝试将优先级低的pod剔除，让优先级高的pod调度成功。 bind：上述步骤完成后，调度器会更新本地缓存，但最后需要将绑定结果提交到etcd中，需要调用Apiserver的Bind接口完成。 以下k8s源码版本为1.13.2 我们去查看kube-scheduler源码，调度器通过list-watch机制，监听集群内Pod的新增、更新、删除事件，调用回调函数。指定nodeName后将不会放入到未调度的podQueue队列中，也就不会走上面这几个阶段。具体可以来到pkgschedulerfactoryfactory.go源码中的NewConfigFactory函数中： 其中在构建pod资源对象新增、更新、删除的回调函数时，分已被调度的和未被调度的回调。 已被调度的回调：已被调度的pod根据FilterFunc中定义的逻辑过滤，nodeName不为空，返回true时，将会走Handler中定义的AddFunc、UpdateFunc、DeleteFunc，这个其实最终不会加入到podQueue中，但需要加入到本地缓存中，因为调度器会维护一份节点上pod列表的缓存。 1234567891011121314151617181920212223242526// scheduled pod cache 已被调度的 args.PodInformer.Informer().AddEventHandler( cache.FilteringResourceEventHandler{ FilterFunc: func(obj interface{}) bool { switch t := obj.(type) { case *v1.Pod: //nodeName不为空,返回true;且返回true时将被走AddFunc、UpdateFunc、DeleteFunc,这个其实最终不会加入到podQueue中 return assignedPod(t) case cache.DeletedFinalStateUnknown: if pod, ok := t.Obj.(*v1.Pod); ok { return assignedPod(pod) } runtime.HandleError(fmt.Errorf(\"unable to convert object %T to *v1.Pod in %T\", obj, c)) return false default: runtime.HandleError(fmt.Errorf(\"unable to handle object in %T: %T\", c, obj)) return false } }, Handler: cache.ResourceEventHandlerFuncs{ AddFunc: c.addPodToCache, UpdateFunc: c.updatePodInCache, DeleteFunc: c.deletePodFromCache, }, }, ) 未被调度的回调：未被调度的pod根据FilterFunc中定义的逻辑过滤，nodeName为空且pod的SchedulerName和该调度器的名称一致时返回true；返回true时，将会走Handler中定义的AddFunc、UpdateFunc、DeleteFunc，这个最终会加入到podQueue中。 1234567891011121314151617181920212223242526// unscheduled pod queue 没有被调度的args.PodInformer.Informer().AddEventHandler( cache.FilteringResourceEventHandler{ FilterFunc: func(obj interface{}) bool { switch t := obj.(type) { case *v1.Pod: //nodeName为空且pod的SchedulerName和该调度器的名称一致时返回true;且返回true时将被加入到pod queue return !assignedPod(t) &amp;&amp; responsibleForPod(t, args.SchedulerName) case cache.DeletedFinalStateUnknown: if pod, ok := t.Obj.(*v1.Pod); ok { return !assignedPod(pod) &amp;&amp; responsibleForPod(pod, args.SchedulerName) } runtime.HandleError(fmt.Errorf(\"unable to convert object %T to *v1.Pod in %T\", obj, c)) return false default: runtime.HandleError(fmt.Errorf(\"unable to handle object in %T: %T\", c, obj)) return false } }, Handler: cache.ResourceEventHandlerFuncs{ AddFunc: c.addPodToSchedulingQueue, UpdateFunc: c.updatePodInSchedulingQueue, DeleteFunc: c.deletePodFromSchedulingQueue, }, },) 手动调度适用场景： 调度器不工作时，可设置nodeName临时救急 ； 可以封装成自己的调度器； 扩展点： 过去几个版本的Daemonset都是由controller直接指定pod的运行节点，不经过调度器。 直到1.11版本，DaemonSet的pod由scheduler调度才作为alpha特性引入 static Pod，这种其实也属于节点固定，但这种Pod局限很大，比如：不能挂载configmaps和secrets等，这个由Admission Controllers控制。 下面简单说一下静态Pod： 静态Pod官网说明：https://kubernetes.io/docs/tasks/configure-pod-container/static-pod/ 静态 pod指在特定的节点上直接通过 kubelet守护进程进行管理，APIServer无法管理。它没有跟任何的控制器进行关联，kubelet 守护进程对它进行监控，如果崩溃了，kubelet 守护进程会重启它。Kubelet 通过APIServer为每个静态 pod 创建 镜像 pod，这些镜像 pod 对于 APIServer是可见的（即kubectl可以查询到这些Pod），但是不受APIServer控制。 具体static pod yaml文件放到哪里，需要在kubelet配置中指定，先找到kubelet配置文件： 1systemctl status kubelet 找到config.yaml文件，里面指定了staticPodPath，kubeadm安装的集群，master节点上的kube-apiserver、kube-scheduler、kube-controller-manager、etcd就是通过static Pod方式部署的。 第五题|Deployment通过命令行，创建两个deployment。 需要集群中有2个节点； 第1个deployment名称为cka-1122-01，使用nginx镜像，有2个pod，并配置该deployment自身的pod之间在节点级别反亲和； 第2个deployment名称为cka-1122-02，使用nginx镜像，有2个pod，并配置该deployment的pod与第1个deployment的pod在节点级别亲和； 第一个deployment：cka-1122-01 1234567891011121314151617181920212223242526272829apiVersion: apps/v1kind: Deploymentmetadata: labels: app: cka-1122-01 name: cka-1122-01spec: replicas: 2 selector: matchLabels: app: cka-1122-01 template: metadata: labels: app: cka-1122-01 spec: containers: - image: nginx name: cka-1122-01 affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: labelSelector: matchExpressions: - key: app operator: In values: - cka-1122-01 topologyKey: \"kubernetes.io/hostname\" 第二个deployment：cka-1122-02 1234567891011121314151617181920212223242526272829apiVersion: apps/v1kind: Deploymentmetadata: labels: app: cka-1122-02 name: cka-1122-02spec: replicas: 2 selector: matchLabels: app: cka-1122-02 template: metadata: labels: app: cka-1122-02 spec: containers: - image: nginx name: cka-1122-02 affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - cka-1122-01 topologyKey: \"kubernetes.io/hostname\" 最终调度结果： 12345NAME READY STATUS RESTARTS AGE IP NODEcka-1122-01-5df9bdf8c9-qwd2v 1/1 Running 0 8m 10.192.4.2 node-1cka-1122-01-5df9bdf8c9-r4rhs 1/1 Running 0 8m 10.192.4.3 node-2 cka-1122-02-749cd4b846-bjhzq 1/1 Running 0 10m 10.192.4.4 node-1cka-1122-02-749cd4b846-rkgpo 1/1 Running 0 10m 10.192.4.5 node-2 解析： 考点：k8s中的高级调度及用法。亲和性和反亲和性调度官方文档：https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ 将 Pod 调度到特定的 Node 上：nodeSelectornodeSelector是节点选择约束的最简单推荐形式。 nodeSelector是PodSpec下的一个字段。它指定键值对的映射。为了使Pod可以在节点上运行，该节点必须具有每个指定的键值对作为label。 语法格式：map[string]string 作用： – 匹配node.labels – 排除不包含nodeSelector中指定label的所有node – 匹配机制 —— 完全匹配 nodeSelector 升级版：nodeAffinity节点亲和性在概念上类似于nodeSelector，它可以根据节点上的标签来限制Pod可以被调度在哪些节点上。 红色框为硬性过滤：排除不具备指定label的node；在预选阶段起作用； 绿色框为软性评分：不具备指定label的node打低分， 降低node被选中的几率；在优选阶段起作用； 与nodeSelector关键差异 – 引入运算符：In，NotIn （labelselector语法） – 支持枚举label可能的取值，如 zone in [az1, az2, az3…] – 支持硬性过滤和软性评分 – 硬性过滤规则支持指定多条件之间的逻辑或运算 – 软性评分规则支持 设置条件权重值 让某些 Pod 分布在同一组 Node 上：podAffinityPod亲和性和反亲和性可以基于已经在节点上运行的Pod上的标签而不是基于节点上的标签，来限制Pod调度的节点。 规则的格式为： 如果该X已经在运行一个或多个满足规则Y的Pod，则该Pod应该（或者在反亲和性的情况下不应该）在X中运行。 Y表示为LabelSelector。X是一个拓扑域，例如节点，机架，云提供者区域，云提供者区域等。 红框硬性过滤： 排除不具备指定pod的node组；在预选阶段起作用； 绿框软性评分： 不具备指定pod的node组打低分， 降低该组node被选中的几率；在优选阶段起作用； 与nodeAffinity的关键差异 – 定义在PodSpec中，亲和与反亲和规则具有对称性 – labelSelector的匹配对象为Pod – 对node分组，依据label-key=topologyKey，每个labelvalue取值为一组 – 硬性过滤规则，条件间只有逻辑与运算 避免某些 Pod 分布在同一组 Node 上：podAntiAffinity 与podAffinity的差异 – 匹配过程相同 – 最终处理调度结果时取反 即 – podAffinity中可调度节点，在podAntiAffinity中为不可调度 – podAffinity中高分节点，在podAntiAffinity中为低分 第六题|deployment的升级回滚、滚动更新策略、roll、set image命令通过命令行，创建1个deployment，副本数为3，镜像为nginx:latest。然后滚动升 级到nginx:1.9.1，再回滚到原来的版本 要求：Deployment的名称为cka-1125，贴出用到的相关命令。 先创建deployment，可以用命令创建： 1kubectl run cka-1125 --image=nginx --replicas=3 也可以用以下yaml：cka-1125.yaml创建 12345678910111213141516171819apiVersion: apps/v1kind: Deploymentmetadata: labels: app: cka-1125 name: cka-1125spec: replicas: 3 selector: matchLabels: app: cka-1125 template: metadata: labels: app: cka-1125 spec: containers: - image: nginx name: cka-1125 创建： 1kubectl apply -f cka-1125.yaml 升级： 12kubectl set image deploy/cka-1125 cka-1125=nginx:1.9.1 --recorddeployment.extensions/cka-1125 image updated 回滚： 1234# 回滚到上一个版本kubectl rollout undo deploy/cka-1125# 回滚到指定版本kubectl rollout undo deploy/cka-1125 --to-revision=2 解析： 官方中set image命令：https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#set set image命令set image命令格式如下： 1kubectl set image (-f FILENAME | TYPE NAME) CONTAINER_NAME_1=CONTAINER_IMAGE_1 ... CONTAINER_NAME_N=CONTAINER_IMAGE_N [--record] –record指定，在annotation中记录当前的kubectl命令。 如果设置为false，则不记录命令。 如果设置为true，则记录命令。 默认为false。 12345678[root@peter test]# kubectl set image deploy/cka-1125 cka-1125=nginx:1.9.1 --recorddeployment.extensions/cka-1125 image updated[root@peter test]# [root@peter test]# kubectl rollout history deploy/cka-1125 deployment.extensions/cka-1125 REVISION CHANGE-CAUSE3 &lt;none&gt;4 kubectl set image deploy/cka-1125 cka-1125=nginx:1.9.1 --record=true 像上面这样，CHANGE-CAUSE中会有升级命令。 set image命令可以对：pod (po), replicationcontroller (rc), deployment (deploy), daemonset (ds), replicaset (rs)，statefulset(sts)进行操作。 roll命令roll命令官方文档：https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#rollout 可以管理deployments、daemonsets、statefulsets资源的回滚： 查询升级历史： 12345[root@peter test]# kubectl rollout history deploy/cka-1125 deployment.extensions/cka-1125 REVISION CHANGE-CAUSE1 &lt;none&gt;2 &lt;none&gt; 查看指定版本的详细信息： 1kubectl rollout history deploy/cka-1125 --revision=3 -o=yaml 回滚到上一个版本： 12[root@peter test]# kubectl rollout undo deploy/cka-1125 deployment.extensions/cka-1125 rolled back 或者回滚到指定版本： 12[root@peter test]# kubectl rollout undo deploy/cka-1125 --to-revision=3deployment.extensions/cka-1125 rolled back 其他roll子命令restart：资源将重新启动；status：展示回滚状态；resume：恢复被暂停的资源。控制器不会控制被暂停的资源。通过恢复资源，可以让控制器再次控制。 resume仅对deployment支持。pause：控制器不会控制被暂停的资源。 使用kubectl rollout resume来恢复暂停的资源。 当前，只有deployment支持被暂停。 滚动更新策略滚动更新官网文档：https://kubernetes.io/docs/concepts/workloads/controllers/deployment/ 123456minReadySeconds: 5strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 1 minReadySecondsKubernetes在等待设置的时间后才进行升级如果没有设置该值，Kubernetes会假设该容器启动起来后就提供服务了如果没有设置该值，在某些极端情况下可能会造成服务服务正常运行 maxSurge 控制滚动更新过程中副本总数超过DESIRED的上限。maxSurge可以是具体的整数，也可以是百分比，向上取整。maxSurge默认值为25%。 例如DESIRED为10，那么副本总数的最大值为roundUp(10 + 10*25%)=13,所以CURRENT为13。 maxUnavaible控制滚动更新过程中，不可用副本占DESIRED的最大比例。maxUnavailable可以是具体的整数，也可以是百分之百，向下取整。默认值为25%。 例如DESIRED为10，那么可用的副本数至少要为 10-roundDown(10*25%)=8所以AVAILABLE为8。 maxSurge越大，初始创建的新副本数量就越多；maxUnavailable越大，初始销毁的旧副本数目就越多。 第8题 | initContainer概念、用法、使用场景简介提供一个pod的yaml，要求添加Init Container，Init Container的作用是创建一个空文件，pod的Containers判断文件是否存在，不存在则退出 123456789101112131415161718192021222324apiVersion: v1kind: Podmetadata: labels: run: cka-1126 name: cka-1126spec: initContainers: - image: busybox name: init-c command: ['sh', '-c', 'touch /tmp/cka-1126'] volumeMounts: - name: workdir mountPath: \"/tmp\" containers: - image: busybox name: cka-1126 command: ['sh', '-c', 'ls /tmp/cka-1126 &amp;&amp; sleep 3600 || exit 1'] volumeMounts: - name: workdir mountPath: \"/tmp\" volumes: - name: workdir emptyDir: {} 主Container的command就是判断文件是否存在，存在则不退出，不存在则退出；也可以用以下if判断： 1command: ['sh', '-c', 'if [ -e /tmp/cka-1126 ];then echo \"file exits\";else echo \"file not exits\" &amp;&amp; exit 1;fi'] 本题的关键点是init容器与主容器需要共同挂载一个名为workdir的目录，init容器在里面创建一个空文件，主容器去检验文件是否存在，检验主要用的是shell的语法； 1command: [&apos;sh&apos;, &apos;-c&apos;, &apos;ls /tmp/cka-1126 &amp;&amp; sleep 3600 || exit 1&apos;] 解析： 这句话意思是：如果ls /tmp/cka-1126返回码为0，即文件存在，将sleep 3600秒；否则exit 1退出； 也可以用shell的if语法判断。 官方文档地址：https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-initialization/https://kubernetes.io/docs/concepts/workloads/pods/init-containers/ 梳理概念初始化容器，顾名思义容器启动的时候，会先启动可一个或多个容器，如果有多个，那么这几个Init Container按照定义的顺序依次执行，一个执行成功，才能执行下一个，只有所有的Init Container执行完后，主容器才会启动。由于一个Pod里的存储卷是共享的，所以Init Container里产生的数据可以被主容器使用到。 Init Container可以在多种K8S资源里被使用到如Deployment、Daemon Set、StatefulSet、Job等，但归根结底都是在Pod启动时，在主容器启动前执行，做初始化工作。 Init 容器支持应用容器的全部字段和特性，包括资源限制、数据卷和安全设置。然而，Init 容器不支持 Readiness Probe，因为它们必须在 Pod 就绪之前运行完成；在资源限制、调度方面也会略有不同。 应用场景等待其它模块Ready：比如有一个应用里面有两个容器化的服务，一个是Web Server，另一个是数据库。其中Web Server需要访问数据库。但是当我们启动这个应用的时候，并不能保证数据库服务先启动起来，所以可能出现在一段时间内Web Server连接数据库错误。为了解决这个问题，我们可以在运行Web Server服务的Pod里使用一个InitContainer，去检查数据库是否准备好，直到数据库可以连接，Init Container才结束退出，然后Web Server容器被启动，发起正式的数据库连接请求。 初始化配置：比如集群里检测所有已经存在的成员节点，为主容器准备好集群的配置信息，这样主容器起来后就能用这个配置信息加入集群；目前在容器化，初始化集群配置文件时经常用到； 提供一种阻塞容器启动的方式：必须在initContainer容器启动成功后，才会运行下一个容器，保证了一组条件运行成功的方式； 其它使用场景：将pod注册到一个中央数据库、下载应用依赖等。 Kubernetes 1.5 版本 开始支持在annotations下用http://pod.beta.kubernetes.io/init-containers申明initContainer，像以下这样。 123456789101112131415161718192021222324apiVersion: v1kind: Podmetadata: name: myapp-pod labels: app: myapp annotations: pod.beta.kubernetes.io/init-containers: '[ { \"name\": \"init-myservice\", \"image\": \"busybox\", \"command\": [\"sh\", \"-c\", \"until nslookup myservice; do echo waiting for myservice; sleep 2; done;\"] }, { \"name\": \"init-mydb\", \"image\": \"busybox\", \"command\": [\"sh\", \"-c\", \"until nslookup mydb; do echo waiting for mydb; sleep 2; done;\"] } ]'spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', 'echo The app is running! &amp;&amp; sleep 3600'] Kubernetes 1.6 版本的新语法将 Init 容器的声明移到 spec 下，但是老的 annotation 语法仍然可以使用。","link":"/2019/11/28/K8s小练习/"}],"tags":[{"name":"java","slug":"java","link":"/tags/java/"},{"name":"java关键字","slug":"java关键字","link":"/tags/java关键字/"},{"name":"test","slug":"test","link":"/tags/test/"},{"name":"Gitlab","slug":"Gitlab","link":"/tags/Gitlab/"},{"name":"Gerrit","slug":"Gerrit","link":"/tags/Gerrit/"},{"name":"面经","slug":"面经","link":"/tags/面经/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"LXC","slug":"LXC","link":"/tags/LXC/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"内核态","slug":"内核态","link":"/tags/内核态/"},{"name":"用户态","slug":"用户态","link":"/tags/用户态/"},{"name":"leetcode","slug":"leetcode","link":"/tags/leetcode/"},{"name":"gitlab update","slug":"gitlab-update","link":"/tags/gitlab-update/"},{"name":"k8s基础","slug":"k8s基础","link":"/tags/k8s基础/"}],"categories":[{"name":"java基础","slug":"java基础","link":"/categories/java基础/"},{"name":"test","slug":"test","link":"/categories/test/"},{"name":"Gitlab","slug":"Gitlab","link":"/categories/Gitlab/"},{"name":"Python原理","slug":"Python原理","link":"/categories/Python原理/"},{"name":"Docker","slug":"Docker","link":"/categories/Docker/"},{"name":"Linux基础","slug":"Linux基础","link":"/categories/Linux基础/"},{"name":"算法","slug":"算法","link":"/categories/算法/"},{"name":"gitlab","slug":"gitlab","link":"/categories/gitlab/"},{"name":"k8s","slug":"k8s","link":"/categories/k8s/"}]}