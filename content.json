{"pages":[],"posts":[{"title":"Gerrit权限控制","text":"最近对gerrit权限了解比较多，简单介绍一下。 Abandon此权限允许用户丢弃一个提交的change。如果用户有push权限，给用户分配此权限的同时用户也被分配了restore a change的权限。 Create Reference此权限管理用户是有可以创建references，branches，tags。此权限一般与普通的push权限一起被分配。 Forge Author伪造发起人权限，此权限允许用户绕过提交时的身份验证（Gerrit默认会匹配提交信息中author或者committer行中的email地址，如果 Email地址不匹配，则不允许提交）。 Forge Committer伪造提交者权限，此权限允许用户绕过提交时的身份验证（Gerrit默认会匹配提交信息中author或者committer行中的email地址，如果 Email地址不匹配，则不允许提交 ）。 Forge Server伪造Gerrit服务器权限，此权限允许在committer行中使用server owner和email Owner此权限允许用户修改香项目的配置，具体如下： 修改项目描述通过ssh的”create-branch”命令创建分支在web UI界面创建/删除branch允许/撤销任何访问权限，包括Owner权限。 Push此分类控制用户被允许怎样推送新commit到Gerrit。 Direct Push所有已存在的branch可以快进到新的commit。创建新分支受“Create Reference”控制，不允许删除已存在的分支，这是最安全的模式（因为commit不可以被丢弃）。 Force option允许已存在的branch被删除。开启此选项可以从项目历史中删除提交记录。此权限主要用来给那些只想用Gerrit的访问控制，不需要Gerrit的代码审查功能的工程使用。 Upload To Code Review此push权限分配在refs/for/refs/heads/BRANCH命名空间上，允许用户提交一个未合并(non-merge)的commit到refs/for/BRANCH命名空间，创建一个新的代码审查change。用户必须能够clone和fetch一个工程才可以提交change，所以用户还必须拥有Read权限。 Push Merge Commits此权限允许用户提交merge commits，它是Push权限的附属物，如果想只允许通过Gerrit做merge操作，那么应该只分配Push仅限而不分配此权限。 Push Annotated Tag此类权限允许用户向工程仓库提交一个annotated tag。通常使用以下两种方式提交： git push ssh://USER@HOST:PORT/PROJECT tag v1.0或者:git push https://HOST/PROJECT tag v1.0 Tags必须被注释（使用git tag -a），必须在refs/tags/下存在，而且必须是新的。一般在工程达到了稳定且可发布的时候会打一个Tag。此权限允许创建一个未签名的Tag。打Tag者的email地址必须与当前用户的一致。如果要提交不是自己打的Tag，则必须同时分配Forge Committer Identity权限。如果要提交轻标签(lightweight tags)分配Create Reference权限给引用/refs/tags/*如果要删除或覆盖一个已存在的tag，分配Push权限并开启Force option。 Push Signed Tag此类权限允许用户向工程仓库提交一个PGP签名的 tag。通常使用以下两种方式提交： git push ssh://USER@HOST:PORT/PROJECT tag v1.0或者:git push https://HOST/PROJECT tag v1.0 Tags必须被注释（使用git tag -a），必须在refs/tags/下存在，而且必须是新的。 Read此类权限控制工程的changes， comments，和code diffs可见性，和是否可通过SSH或HTTP访问Git。如果在单独工程的ACL中设置的此权限，那么全局ACL中的设置将不起作用。 Rebase此类仅限允许用户通过web页面的“Rebase Change”按钮衍合（Rebase）修改 Remove Reviewer此类权限允许用户在一个change的reviewers list中移除其他用户。change所属者可以移除0分或负分的reviewers（即使没有此权限）。项目所有者和网站管理员可以移除所有reviewers（即使没有此权限）。没有此权限的用户只可以移除自己。 Review LabelsSubmit此类权限允许用户提交changes。提交一个change会使该change尽可能快的合并到目的分支，使其作为项目历史永久的一部分。为了提交change,所有的labels都必须允许提交，并且不能block它。如果要快速提交一个push上的change，用户需要在refs/for/(e.g. on refs/for/refs/heads/master)有此权限。 Submit(On Behalf Of)此类权限允许有Submit权限的用户代表其他用户提交change。在project.config文件中，此权限被命名为submitAs。 View Drafts此类权限允许用户查看其他用户提交的drafts changeschange所用者和任何明确添加的reviewers也可以查看（即使没用此权限） Publish Drafts此类权限允许用户发布其他用户提交的drafts changeschange所用者和任何明确添加的reviewers也可以查看（即使没用此权限） Delete Drafts此类权限允许用户删除其他用户提交的drafts changeschange所用者和任何明确添加的reviewers也可以查看（即使没用此权限） Edit Topic Name允许用户编辑提交到review的change的话题名。change所用者，分支所用者，项目所用者和网站管理员都可以编辑此话题名（即使没有此权限）。“Force Edit”标识控制是否可以编辑已关闭的change标题，如果此标识设置只能编辑open changes，则不可以编辑已关闭的change 标题。 Edit Hashtags允许用户在提交到reviews的changes上添加或移除hashtags。change所用者和任何明确添加的reviewers也可以查看（即使没用此权限）","link":"/2020/03/03/Gerrit权限控制/"},{"title":"MySQL相关日志介绍","text":"简单介绍一下MySQL使用的相关日志，面试可能会问哦！ 一、什么是binlogbinlog其实在日常的开发中是听得很多的，因为很多时候数据的更新就依赖着binlog。举个很简单的例子：我们的数据是保存在数据库里边的嘛，现在我们对某个商品的某个字段的内容改了（数据库变更），而用户检索的出来数据是走搜索引擎的。为了让用户能搜到最新的数据，我们需要把引擎的数据也改掉。一句话：数据库的变更，搜索引擎的数据也需要变更。于是，我们就会监听binlog的变更，如果binlog有变更了，那我们就需要将变更写到对应的数据源。 什么是binlog？ binlog记录了数据库表结构和表数据变更，比如update/delete/insert/truncate/create。它不会记录select（因为这没有对表没有进行变更）binlog长什么样？ binlog我们可以简单理解为：存储着每条变更的SQL语句（当然从下面的图看来看，不止SQL，还有XID「事务Id」等等）binlog一般用来做什么 主要有两个作用：复制和恢复数据MySQL在公司使用的时候往往都是一主多从结构的，从服务器需要与主服务器的数据保持一致，这就是通过binlog来实现的数据库的数据被干掉了，我们可以通过binlog来对数据进行恢复。因为binlog记录了数据库表的变更，所以我们可以用binlog进行复制（主从复制)和恢复数据。 二、什么是redo log假设我们有一条sql语句：update user_table set name='java3y' where id = '3'MySQL执行这条SQL语句，肯定是先把id=3的这条记录查出来，然后将name字段给改掉。这没问题吧？实际上Mysql的基本存储结构是页(记录都存在页里边)，所以MySQL是先把这条记录所在的页找到，然后把该页加载到内存中，将对应记录进行修改。现在就可能存在一个问题：如果在内存中把数据改了，还没来得及落磁盘，而此时的数据库挂了怎么办？显然这次更改就丢了。如果每个请求都需要将数据立马落磁盘之后，那速度会很慢，MySQL可能也顶不住。所以MySQL是怎么做的呢？MySQL引入了redo log，内存写完了，然后会写一份redo log，这份redo log记载着这次在某个页上做了什么修改。其实写redo log的时候，也会有buffer，是先写buffer，再真正落到磁盘中的。至于从buffer什么时候落磁盘，会有配置供我们配置。写redo log也是需要写磁盘的，但它的好处就是顺序IO（我们都知道顺序IO比随机IO快非常多）。所以，redo log的存在为了：当我们修改的时候，写完内存了，但数据还没真正写到磁盘的时候。此时我们的数据库挂了，我们可以根据redo log来对数据进行恢复。因为redo log是顺序IO，所以写入的速度很快，并且redo log记载的是物理变化（xxxx页做了xxx修改），文件的体积很小，恢复速度很快。 三、binlog和redo log看到这里，你可能会想：binlog和redo log 这俩也太像了吧，都是用作”恢复“的。其实他俩除了”恢复”这块是相似的，很多都不一样，下面看我列一下。存储的内容上： binlog记载的是update/delete/insert这样的SQL语句 而redo log记载的是物理修改的内容（xxxx页修改了xxx） 所以在搜索资料的时候会有这样的说法：redo log记录的是数据的物理变化，binlog 记录的是数据的逻辑变化功能redo log的作用是为持久化而生的。写完内存，如果数据库挂了，那我们可以通过redo log来恢复内存还没来得及刷到磁盘的数据，将redo log加载到内存里边，那内存就能恢复到挂掉之前的数据了。binlog的作用是复制和恢复而生的。主从服务器需要保持数据的一致性，通过binlog来同步数据。如果整个数据库的数据都被删除了，binlog存储着所有的数据变更情况，那么可以通过binlog来对数据进行恢复。又看到这里，你会想：”如果整个数据库的数据都被删除了，那我可以用redo log的记录来恢复吗？“不能因为功能的不同，redo log 存储的是物理数据的变更，如果我们内存的数据已经刷到了磁盘了，那redo log的数据就无效了。所以redo log不会存储着历史所有数据的变更，文件的内容会被覆盖的。 binlog和redo log写入的细节redo log是MySQL的InnoDB引擎所产生的。binlog无论MySQL用什么引擎，都会有的。InnoDB是有事务的，事务的四大特性之一：持久性就是靠redo log来实现的（如果写入内存成功，但数据还没真正刷到磁盘，如果此时的数据库挂了，我们可以靠redo log来恢复内存的数据，这就实现了持久性）。上面也提到，在修改的数据的时候，binlog会记载着变更的类容，redo log也会记载着变更的内容。（只不过一个存储的是物理变化，一个存储的是逻辑变化）。 那他们的写入顺序是什么样的呢？redo log事务开始的时候，就开始记录每次的变更信息，而binlog是在事务提交的时候才记录。于是新有的问题又出现了： 我写其中的某一个log，失败了，那会怎么办现在我们的前提是先写redo log，再写binlog，我们来看看：如果写redo log失败了，那我们就认为这次事务有问题，回滚，不再写binlog。如果写redo log成功了，写binlog，写binlog写一半了，但失败了怎么办？我们还是会对这次的事务回滚，将无效的binlog给删除（因为binlog会影响从库的数据，所以需要做删除操作）如果写redo log和binlog都成功了，那这次算是事务才会真正成功。简单来说：MySQL需要保证redo log和binlog的数据是一致的，如果不一致，那就乱套了。如果redo log写失败了，而binlog写成功了。那假设内存的数据还没来得及落磁盘，机器就挂掉了。那主从服务器的数据就不一致了。（从服务器通过binlog得到最新的数据，而主服务器由于redo log没有记载，没法恢复数据）如果redo log写成功了，而binlog写失败了。那从服务器就拿不到最新的数据了。MySQL通过两阶段提交来保证redo log和binlog的数据是一致的。过程： 阶段1：InnoDBredo log 写盘，InnoDB事务进入prepare状态 阶段2：binlog 写盘，InooDB 事务进入commit状态，每个事务binlog的末尾，会记录一个XID event，标志着事务是否提交成功，也就是说，恢复过程中，binlog 最后一个 XID event 之后的内容都应该被 purge。 四、什么是undo logundo log主要有两个作用：回滚和多版本控制(MVCC)在数据修改的时候，不仅记录了redo log，还记录undo log，如果因为某些原因导致事务失败或回滚了，可以用undo log进行回滚undo log主要存储的也是逻辑日志，比如我们要insert一条数据了，那undo log会记录的一条对应的delete日志。我们要update一条记录时，它会记录一条对应相反的update记录。这也应该容易理解，毕竟回滚嘛，跟需要修改的操作相反就好，这样就能达到回滚的目的。因为支持回滚操作，所以我们就能保证：“一个事务包含多个操作，这些操作要么全部执行，要么全都不执行”。【原子性】因为undo log存储着修改之前的数据，相当于一个前版本，MVCC实现的是读写不阻塞，读的时候只要返回前一个版本的数据就行了。 这篇文章把binlog /redo log/undo log最核心的知识给讲了，还有一些细节性的东西可以自行去补充（比如binlog有几种的模式，以及文章提到的刷盘策略等等）参考链接：https://zhuanlan.zhihu.com/p/112106862https://www.jianshu.com/p/4bcfffb27ed5https://www.cnblogs.com/myseries/p/10728533.html","link":"/2020/03/16/MySQL相关日志介绍/"},{"title":"RAID简单介绍","text":"简单介绍一下常用的RAID方式，一共有0~6一共7种，这其中RAID 0、RAID1、RAID 5和RAID6比较常用。 RAID 0如果你有n块磁盘，原来只能同时写一块磁盘，写满了再下一块，做了RAID 0之后，n块可以同时写，速度提升很快，但由于没有备份，可靠性很差。n最少为2。 RAID 1正因为RAID 0太不可靠，所以衍生出了RAID 1。如果你有n块磁盘，把其中n/2块磁盘作为镜像磁盘，在往其中一块磁盘写入数据时，也同时往另一块写数据。坏了其中一块时，镜像磁盘自动顶上，可靠性最佳，但空间利用率太低。n最少为2。 RAID 3RAID 3是若你有n块盘，其中1块盘作为校验盘，剩余n-1块盘相当于作RAID 0同时读写，当其中一块盘坏掉时，可以通过校验码还原出坏掉盘的原始数据。这个校验方式比较特别，奇偶检验，1 XOR 0 XOR 1=0，0 XOR 1 XOR 0=1，最后的数据时校验数据，当中间缺了一个数据时，可以通过其他盘的数据和校验数据推算出来。但是这有个问题，由于n-1块盘做了RAID 0，每一次读写都要牵动所有盘来为它服务，而且万一校验盘坏掉就完蛋了。最多允许坏一块盘。n最少为3。 RAID 5在RAID 3的基础上有所区别，同样是相当于是1块盘的大小作为校验盘，n-1块盘的大小作为数据盘，但校验码分布在各个磁盘中，不是单独的一块磁盘，也就是分布式校验盘，这样做好处多多。最多坏一块盘。n最少为3。 RAID6在RAID 5的基础上，又增加了一种校验码，和解方程似的，一种校验码一个方程，最多有两个未知数，也就是最多坏两块盘。","link":"/2020/03/14/RAID简单介绍/"},{"title":"Java中static, final, static final的区别","text":"通常情况下， 类成员需要通过它的类的对象访问，如果一个成员被声明为static，它能够在它的类的任何对象创建之前被访问， 而不用引用任何对象。 声明为static的一个类变量或方法，所有的该类的实例都会共享这个static变量或方法。 staticstatic修饰变量 静态变量在内存中只有一份， jvm只为静态变量分配一次内存，随着类的加载而加载到静态方法区内存中。由于静态变量属于类，和类的实例无关， 所以可以直接通过类名进行访问。 对于成员变量，每创建一个该类的实例就会创建该成员变量的一个拷贝，分配一次内存，由于成员变量是和类的实例绑定的，所以不能直接通过类名对它进行访问。 static修饰方法 只能调用其他的static方法 只能访问static数据 不能以任何方式引用this 和super 静态方法可以直接通过类名调用， 任何该类的实例也可以调用它的静态方法， 所以静态方法不能用this或者super。 static 方法独立于任何实例， 所以static方法必须被实现，不能是抽象的absract，在static方法里引用任何的实例变量都是违法的。 static 修饰类普通类不允许被声明为静态， 只有内部类才可以。被static修饰的内部类可以作为一个普通类来使用， 而不需实例一个外部类（不需要new，直接静态加载）。 内部类没有使用static关键字，不能直接创建实例。 不使用static修饰内部类 12345678910111213public class OuterClass { public class InnerClass{ InnerClass(){} }}public class TestStaticClass { public static void main(String[] args) { // OutClass需要先生成一个实例 OuterClass oc = new OuterClass(); oc.new InnerClass(); }} 使用static修饰内部类 123456789101112public class OuterClass { public static class InnerClass{ InnerClass(){} }}public class TestStaticClass { public static void main(String[] args) { // OutClass 不需要生成实例 new OuterClass.InnerClass(); }} static修饰代码块123static { System.out.println(\"test\");} 它独立于类成员，可以有多个， jvm 加载类的时候会执行这些静态代码块， 如果有static代码块多个，jvm会按照他们在类中出现的顺序执行且每个只执行一次。可以通过静态代码块对static变量进行赋值。 finalfinal可以修饰非抽象类， 非抽象类成员方法和变量 final修饰变量一个变量可以声明为final， 目的是阻止它的内容被修改， 这意味着声明final变量的时候， 必须对其进行初始化，这种用法有点类似于c++的const。 通常，我们会用 final定义一些常量 ， 如 123final float PI=3.14final boolean SUCCESS = true..... 按照编码约定， final变量的所有字符选择大写，final修饰的变量实际中不占用内存， 它实质上是一个常数。 final修饰方法被final修饰的方法可以被子类继承， 但不能被子类的方法覆盖。 如果一个类不想让其子类覆盖它的某个成员方法， 就可以用 final关键字修饰该方法。 final不能修饰构造方法。 由于父类中private 成员方法不能被子类覆盖， 所有由private修饰的方法默认也是final的。 使用final修饰成员方法除了不想让子类覆盖外， 还有一个原因就是高效，Java编译器在遇到final修饰的方法的时候会转入内嵌机制， 提高执行效率。 内嵌机制 ，类似于c++ inline, 调用方法的时候直接将方法的主题插入到调用处， 而不用去访问类或者对象， 这样会提高50%左右效率。然而，如果方法主体比较庞大， 且多处被调用将导致主体代码膨胀， 同时也产生效率问题， 所以需要慎用。 final修饰类final修饰的类不能被继承。 final和static同时使用同时使用 final和static修饰类成员， 该类成员拥有二者特性。 1static final int LIMIT=100; // LIMIT表示全局常量 如果是方法的话，方法可以被继承， 可以通过类名被访问， 但是不能被子类覆盖。 对于一些用final和static修饰的容器类型（比如，ArrayList、HashMap）的实例变量，不可以改变容器变量本身，但可以修改容器中存放的对象。","link":"/2019/11/04/final-static-finalwithstatic/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2019/11/04/hello-world/"},{"title":"迁移一个Gitlab项目至Gerrit","text":"Gerrit采用的分支方法与标准Git不同。在Gerrit中，基本上每个分支都是受保护的分支，但是对分支的需求要低得多，因为Gerrit会根据需要自动创建它们。作为开发人员，我可以在master分支上工作，当我推送到origin时，Gerrit会自动在Origin中创建一个称为refs/for/master的“虚拟”分支，并将更改放入该分支。这类似于在问题中创建MR时GitLab合并请求创建分支的过程。区别在于，有refs/for/master是虚拟的，所以有很多-我有refs/for/master，有refs/for/master，每个推送的人在原始仓库中都有一个。更改将保留在该虚拟分支上，直到按照规则完成审核为止，这时它将合并，并且虚拟分支将被删除。Gerrit支持分支上的读写特权，而不仅仅是写特权。 目前也没有批量迁移gitlab项目至gerrit的工具，比较保险的方法如下： 1)使用UI在Gerrit中创建存储库(或要求Gerrit管理员执行此操作) 2)使用” –bare”选项克隆Gitlab存储库 1git clone --bare GITLAB-URL 3)添加Gerrit remote 12cd REPO-NAMEgit remote add gerrit GERRIT-URL 4)将所有提交，分支和标签推送到Gerrit 12git push --all gerritgit push --tags gerrit 5)删除bare repo 12cd ..rm -rf REPO-NAME 以上请确保已经添加了公钥到新的机器上，以下是clone时的区别： git clone origin-url（非裸）：您将得到所有复制的标签，一个本地分支master (HEAD)追踪远程分支origin/master和远程分支origin/next，origin/pu和origin/maint。设置了跟踪分支，这样如果你做了类似的事情git fetch origin，它们就会像你期望的那样被提取。任何远程分支（在克隆的远程中）和其他引用都被完全忽略。 git clone –bare origin-url：您将获得全部复制的标签，地方分支机构master (HEAD)，next，pu，和maint，没有远程跟踪分支。也就是说，所有分支都按原样复制，并且它设置为完全独立，不期望再次获取。任何远程分支（在克隆的远程中）和其他引用都被完全忽略。 git clone –mirror origin-url：这些引用中的每一个都将按原样复制。你会得到所有的标签，地方分支机构master (HEAD)，next，pu，和maint，远程分支机构devA/master和devB/master其他裁判refs/foo/bar和refs/foo/baz。一切都与克隆的遥控器完全一样。设置远程跟踪，以便在运行时，git remote update所有引用都将从原点覆盖，就像您刚删除镜像并重新克隆它一样。正如文档最初所说，它是一面镜子。它应该是功能相同的副本，可与原始版本互换。 经测试，clone选项 –bare和–mirror均可成功实现迁移，不过还是建议用–bare克隆裸库。","link":"/2020/02/27/moveagitlabrepotogerrit/"},{"title":"为什么说Python是伪多线程","text":"学过操作系统的同学都知道，线程是现代操作系统底层一种轻量级的多任务机制。一个进程空间中可以存在多个线程，每个线程代表一条控制流，共享全局进程空间的变量，又有自己私有的内存空间。多个线程可以同时执行。此处的“同时”，在较早的单核架构中表现为“伪并行”，即让线程以极短的时间间隔交替执行，从人的感觉上看它们就像在同时执行一样。但由于仅有一个运算单元，当线程皆执行计算密集型任务时，多线程可能会出现 1 + 1 &gt; 2 的反效果。 GIL 是什么GIL 的全名是 the Global Interpreter Lock （全局解释锁），是常规 python 解释器（当然，有些解释器没有）的核心部件。我们看看官方的解释： The Python interpreter is not fully thread-safe. In order to support multi-threaded Python programs, there’s a global lock, called the global interpreter lock or GIL, that must be held by the current thread before it can safely access Python objects. 这是一个用于保护 Python 内部对象的全局锁（在进程空间中唯一），保障了解释器的线程安全。 这里用一个形象的例子来说明 GIL 的必要性（对资源抢占问题非常熟悉的可以跳过不看）： 我们把整个进程空间看做一个车间，把线程看成是多条不相交的流水线，把线程控制流中的字节码看作是流水线上待处理的物品。Python 解释器是工人，整个车间仅此一名。操作系统是一只上帝之手，会随时把工人从一条流水线调到另一条——这种“随时”是不由分说的，即不管处理完当前物品与否。若没有 GIL。假设工人正在流水线 A 处理 A1 物品，根据 A1 的需要将房间温度（一个全局对象）调到了 20 度。这时上帝之手发动了，工人被调到流水线 B 处理 B1 物品，根据 B1 的需要又将房间温度调到了 50 度。这时上帝之手又发动了，工人又调回 A 继续处理 A1。但此时 A1 暴露在了 50 度的环境中，安全问题就此产生了。而 GIL 相当于一条锁链，一旦工人开始处理某条流水线上的物品，GIL 便会将工人和该流水线锁在一起。而被锁住的工人只会处理该流水线上的物品。就算突然被调到另一条流水线，他也不会干活，而是干等至重新调回原来的流水线。这样每个物品在被处理的过程中便总是能保证全局环境不会突变。 该怎么提升效率呢GIL 是 Python 解释器正确运行的保证，Python 语言本身没有提供任何机制访问它。但在特定场合，我们仍有办法降低它对效率的影响。 使用多进程线程间会竞争资源是因为它们共享同一个进程空间，但进程的内存空间是独立的，自然也就没有必要使用解释锁了。 许多人非常忌讳使用多进程，理由是进程操作（创建、切换）的时间开销太大了，而且会占用更多的内存。这种担心其实没有必要——除非是对并发量要求很高的应用（如服务器），多进程增加的时空开销其实都在可以接受的范围中。更何况，我们可以使用进程池减少频繁创建进程带来的开销。 使用C拓展GIL 并不是完全的黑箱，CPython 在解释器层提供了控制 GIL 的开关——这就是 Py_BEGIN_ALLOW_THREADS 和 Py_END_ALLOW_THREADS 宏。这一对宏允许你在自定义的 C 扩展中释放 GIL，从而可以重新利用多核的优势。 沿用上面的例子，自定义的 C 扩展函数好比是流水线上一个特殊的物品。这个物品承诺自己不依赖全局环境，同时也不会要求工人去改变全局环境。同时它带有 Py_BEGIN_ALLOW_THREADS 和 Py_END_ALLOW_THREADS 两个机关，前者能砍断 GIL 锁链，这样工人被调度走后不需要干等，而是可以直接干活；后者则将锁链重新锁上，保证操作的一致性。 面试的高频点有同学可能会奇怪了：我在使用 python 多线程写爬虫时可从来没有这种问题啊——用 4 个线程下载 4 个页面的时间与单线程下载一个页面的时间相差无几。 这里就要谈到 GIL 的第二种释放时机了。除了调用 Py_BEGIN_ALLOW_THREADS，解释器还会在发生阻塞 IO（如网络、文件）时释放 GIL。发生阻塞 IO 时，调用方线程会被挂起，无法进行任何操作，直至内核返回；IO 函数一般是原子性的，这确保了调用的线程安全性。因此在大多数阻塞 IO 发生时，解释器没有理由加锁。 以爬虫为例：当 Thread1 发起对 Page1 的请求后，Thread1 会被挂起，此时 GIL 释放。当控制流切换至 Thread2 时，由于没有 GIL，不必干等，而是可以直接请求 Page2……如此一来，四个请求可以认为是几乎同时发起的。时间开销便与单线程请求一次一样。 有人反对使用阻塞IO，因为若想更好利用阻塞时的时间，必须使用多线程或进程，这样会有很大的上下文切换开销，而非阻塞 IO + 协程显然是更经济的方式。但当若干任务之间没有偏序关系时，一个任务阻塞是可以接受的（毕竟不会影响到其他任务的执行），同时也会简化程序的设计。而在一些通信模型（如 Publisher-Subscriber）中，“阻塞”是必要的语义。 多个阻塞IO需要多条非抢占式的控制流来承载，这些工作交给线程再合适不过了。","link":"/2019/11/22/python-GIL/"},{"title":"Docker与LXC简介","text":"Docker 是一个开源的应用容器引擎，基于 Go 语言 并遵从Apache2.0协议开源。Docker 可以让开发者打包他们的应用以及依赖包到一个轻量级、可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。容器是完全使用沙箱机制，相互之间不会有任何接口（类似 iPhone 的 app）,更重要的是容器性能开销极低。 Linux Container容器是一种内核虚拟化技术，可以提供轻量级的虚拟化，以便隔离进程和资源。 Docker的实现Docker 底层的核心技术包括 Linux 上的命名空间（Namespaces）、控制组（Control groups）、Union 文件系统（Union file systems）和容器格式（Container format）。 我们知道，传统的虚拟机通过在宿主主机中运行 hypervisor 来模拟一整套完整的硬件环境提供给虚拟机的操作系统。虚拟机系统看到的环境是可限制的，也是彼此隔离的。 这种直接的做法实现了对资源最完整的封装，但很多时候往往意味着系统资源的浪费。 例如，以宿主机和虚拟机系统都为 Linux 系统为例，虚拟机中运行的应用其实可以利用宿主机系统中的运行环境。 在操作系统中，包括内核、文件系统、网络、PID、UID、IPC、内存、硬盘、CPU 等等，所有的资源都是应用进程直接共享的。 要想实现虚拟化，除了要实现对内存、CPU、网络IO、硬盘IO、存储空间等的限制外，还要实现文件系统、网络、PID、UID、IPC等等的相互隔离。 前者相对容易实现一些，后者则需要宿主机系统的深入支持。 随着 Linux 系统对于命名空间功能的完善实现，程序员已经可以实现上面的所有需求，让某些进程在彼此隔离的命名空间中运行。大家虽然都共用一个内核和某些运行时环境（例如一些系统命令和系统库），但是彼此却看不到，都以为系统中只有自己的存在。这种机制就是容器（Container），利用命名空间来做权限的隔离控制，利用cgroups来做资源分配。 命名空间是 Linux 内核一个强大的特性。每个容器都有自己单独的命名空间，运行在其中的应用都像是在独立的操作系统中运行一样。命名空间保证了容器之间彼此互不影响。 pid 命名空间不同用户的进程就是通过 pid 命名空间隔离开的，且不同命名空间中可以有相同 pid。所有的 LXC 进程在 Docker 中的父进程为Docker进程，每个 LXC 进程具有不同的命名空间。同时由于允许嵌套，因此可以很方便的实现嵌套的 Docker 容器。 net 命名空间有了 pid 命名空间, 每个命名空间中的 pid 能够相互隔离，但是网络端口还是共享 host 的端口。网络隔离是通过 net 命名空间实现的， 每个 net 命名空间有独立的 网络设备, IP 地址, 路由表, /proc/net 目录。这样每个容器的网络就能隔离开来。Docker 默认采用 veth 的方式，将容器中的虚拟网卡同 host 上的一 个Docker 网桥 docker0 连接在一起。 ipc 命名空间容器中进程交互还是采用了 Linux 常见的进程间交互方法(interprocess communication - IPC), 包括信号量、消息队列和共享内存等。然而同 VM 不同的是，容器的进程间交互实际上还是 host 上具有相同 pid 命名空间中的进程间交互，因此需要在 IPC 资源申请时加入命名空间信息，每个 IPC 资源有一个唯一的 32 位 id。 mnt 命名空间类似 chroot，将一个进程放到一个特定的目录执行。mnt 命名空间允许不同命名空间的进程看到的文件结构不同，这样每个命名空间 中的进程所看到的文件目录就被隔离开了。同 chroot 不同，每个命名空间中的容器在 /proc/mounts 的信息只包含所在命名空间的 mount point。 uts 命名空间UTS(“UNIX Time-sharing System”) 命名空间允许每个容器拥有独立的 hostname 和 domain name, 使其在网络上可以被视作一个独立的节点而非 主机上的一个进程。 user 命名空间每个容器可以有不同的用户和组 id, 也就是说可以在容器内用容器内部的用户执行程序而非主机上的用户。 注：更多关于 Linux 上命名空间的信息，请阅读 这篇文章。 Linux Container简介LXC为Linux Container的简写。可以提供轻量级的虚拟化，以便隔离进程和资源，而且不需要提供指令解释机制以及全虚拟化的其他复杂性。相当于C 中的NameSpace。容器有效地将由单个操作系统管理的资源划分到孤立的组中，以更好地在孤立的组之间平衡有冲突的资源使用需求。与传统虚拟化技术相比，它的优势在于： （1）与宿主机使用同一个内核，性能损耗小； （2）不需要指令级模拟； （3）不需要即时(Just-in-time)编译； （4）容器可以在CPU核心的本地运行指令，不需要任何专门的解释机制； （5）避免了准虚拟化和系统调用替换中的复杂性； （6）轻量级隔离，在隔离的同时还提供共享机制，以实现容器与宿主机的资源共享。 换句话说，Linux Container是一种轻量级的虚拟化的手段。","link":"/2019/11/18/simple-docker/"},{"title":"用户态和内核态的区别","text":"由于需要限制不同的程序之间的访问能力, 防止他们获取别的程序的内存数据, 或者获取外围设备的数据, 并发送到网络, CPU划分出两个权限等级—用户态和内核态。 什么是用户态和内核态内核态：cpu可以访问内存的所有数据，包括外围设备，例如硬盘，网卡，cpu也可以将自己从一个程序切换到另一个程序。 用户态：只能受限的访问内存，且不允许访问外围设备，占用cpu的能力被剥夺，cpu资源可以被其他程序获取。 用户态与内核态的切换所有用户程序都是运行在用户态的, 但是有时候程序确实需要做一些内核态的事情, 例如从硬盘读取数据, 或者从键盘获取输入等. 而唯一可以做这些事情的就是操作系统, 所以此时程序就需要先操作系统请求以程序的名义来执行这些操作。 内核态与用户态是操作系统的两种运行级别,跟intel cpu没有必然的联系,intel cpu提供Ring0-Ring3三种级别的运行模式，Ring0级别最高，Ring3最低。Linux使用了Ring3级别运行用户态，Ring0作为内核态，没有使用Ring1和Ring2。Ring3状态不能访问Ring0的地址空间，包括代码和数据。Linux进程的4GB地址空间，3G-4G部 分大家是共享的，是内核态的地址空间，这里存放在整个内核的代码和所有的内核模块，以及内核所维护的数据。用户运行一个程序，该程序所创建的进程开始是运 行在用户态的，如果要执行文件操作，网络数据发送等操作，必须通过write，send等系统调用，这些系统调用会调用内核中的代码来完成操作，这时，必须切换到Ring0，然后进入3GB-4GB中的内核地址空间去执行这些代码完成操作，完成后，切换回Ring3，回到用户态。这样，用户态的程序就不能 随意操作内核地址空间，具有一定的安全保护作用。 这时需要一个这样的机制: 用户态程序切换到内核态, 但是不能控制在内核态中执行的指令。 这种机制叫系统调用, 在CPU中的实现称之为陷阱指令(Trap Instruction)当一个任务（进程）执行系统调用而陷入内核代码中执行时，我们就称进程处于内核运行态（或简称为内核态）。此时处理器处于特权级最高的（0级）内核代码中执行。当进程处于内核态时，执行的内核代码会使用当前进程的内核栈。每个进程都有自己的内核栈。当进程在执行用户自己的代码时，则称其处于用户运行态（用户态）。即此时处理器在特权级最低的（3级）用户代码中运行。当正在执行用户程序而突然被中断程序中断时，此时用户程序也可以象征性地称为处于进程的内核态。因为中断处理程序将使用当前进程的内核栈。这与处于内核态的进程的状态有些类似。 用户态切换到内核态的3种方式系统调用这是用户态进程主动要求切换到内核态的一种方式，用户态进程通过系统调用申请使用操作系统提供的服务程序完成工作，比如前例中fork()实际上就是执行了一个创建新进程的系统调用。而系统调用的机制其核心还是使用了操作系统为用户特别开放的一个中断来实现，例如Linux的int 80h中断。 异常当CPU在执行运行在用户态下的程序时，发生了某些事先不可知的异常，这时会触发由当前运行进程切换到处理此异常的内核相关程序中，也就转到了内核态，比如缺页异常。 外围设备的中断当外围设备完成用户请求的操作后，会向CPU发出相应的中断信号，这时CPU会暂停执行下一条即将要执行的指令转而去执行与中断信号对应的处理程序，如果先前执行的指令是用户态下的程序，那么这个转换的过程自然也就发生了由用户态到内核态的切换。比如硬盘读写操作完成，系统会切换到硬盘读写的中断处理程序中执行后续操作等。 这3种方式是系统在运行时由用户态转到内核态的最主要方式，其中系统调用可以认为是用户进程主动发起的，异常和外围设备中断则是被动的。 具体的切换操作从触发方式上看，可以认为存在前述3种不同的类型，但是从最终实际完成由用户态到内核态的切换操作上来说，涉及的关键步骤是完全一致的，没有任何区别，都相当于执行了一个中断响应的过程，因为系统调用实际上最终是中断机制实现的，而异常和中断的处理机制基本上也是一致的，关于它们的具体区别这里不再赘述。关于中断处理机制的细节和步骤这里也不做过多分析，涉及到由用户态切换到内核态的步骤主要包括： 123451. 用户态程序将一些数据值放在寄存器中, 或者使用参数创建一个堆栈(stack frame), 以此表明需要操作系统提供的服务。2. 用户态程序执行陷阱指令。3. CPU切换到内核态, 并跳到位于内存指定位置的指令, 这些指令是操作系统的一部分, 他们具有内存保护, 不可被用户态程序访问4. 这些指令称之为陷阱(trap)或者系统调用处理器(system call handler). 他们会读取程序放入内存的数据参数, 并执行程序请求的服务。5. 系统调用完成后, 操作系统会重置CPU为用户态并返回系统调用的结果。 至于说保护模式，是说通过内存页表操作等机制，保证进程间的地址空间不会互相冲突，一个进程的操作不会修改另一个进程的地址空间中的数据。","link":"/2019/11/15/yonghutai-neihetai/"},{"title":"打家劫舍","text":"总结一下LeetCode 打家劫舍的专题。嘻嘻 打家劫舍你是一个专业的小偷，计划偷窃沿街的房屋。每间房内都藏有一定的现金，影响你偷窃的唯一制约因素就是相邻的房屋装有相互连通的防盗系统，如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警。 给定一个代表每个房屋存放金额的非负整数数组，计算你在不触动警报装置的情况下，能够偷窃到的最高金额。 示例 1: 输入: [1,2,3,1]输出: 4解释: 偷窃 1 号房屋 (金额 = 1) ，然后偷窃 3 号房屋 (金额 = 3)。 偷窃到的最高金额 = 1 + 3 = 4 。 示例 2: 输入: [2,7,9,3,1]输出: 12解释: 偷窃 1 号房屋 (金额 = 2), 偷窃 3 号房屋 (金额 = 9)，接着偷窃 5 号房屋 (金额 = 1)。 偷窃到的最高金额 = 2 + 9 + 1 = 12 。 思路：动态规划，要不这家或者前一家+后一家 max(nums[i]+dp[i-2],dp[i-1] 12345678910class Solution: def rob(self, nums: List[int]) -&gt; int: if(len(nums)==0): return 0 if(len(nums)==1): return nums[0] dp = [] dp.append(nums[0]) dp.append(max(nums[0],nums[1])) for i in range(2,len(nums)): dp.append(max(nums[i]+dp[i-2],dp[i-1])) return dp[len(nums)-1] 打家劫舍 II你是一个专业的小偷，计划偷窃沿街的房屋，每间房内都藏有一定的现金。这个地方所有的房屋都围成一圈，这意味着第一个房屋和最后一个房屋是紧挨着的。同时，相邻的房屋装有相互连通的防盗系统，如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警。 给定一个代表每个房屋存放金额的非负整数数组，计算你在不触动警报装置的情况下，能够偷窃到的最高金额。 示例 1: 输入: [2,3,2]输出: 3解释: 你不能先偷窃 1 号房屋（金额 = 2），然后偷窃 3 号房屋（金额 = 2）, 因为他们是相邻的。示例 2: 输入: [1,2,3,1]输出: 4解释: 你可以先偷窃 1 号房屋（金额 = 1），然后偷窃 3 号房屋（金额 = 3）。 偷窃到的最高金额 = 1 + 3 = 4 。 思路：和第一道题一样，不过分两种情况讨论，去掉第一家或者去掉最后一家，最后取大。 在不偷窃第一个房子的情况下（即 nums[1:]nums[1:]），最大金额是 p1 在不偷窃最后一个房子的情况下（即 nums[:n-1]nums[:n−1]），最大金额是 p2 综合偷窃最大金额： 为以上两种情况的较大值，即 max(p1,p2) 12345678class Solution: def rob(self, nums: [int]) -&gt; int: def my_rob(nums): cur, pre = 0, 0 for num in nums: cur, pre = max(pre + num, cur), cur return cur return max(my_rob(nums[:-1]),my_rob(nums[1:])) if len(nums) != 1 else nums[0] 打家劫舍 III在上次打劫完一条街道之后和一圈房屋后，小偷又发现了一个新的可行窃的地区。这个地区只有一个入口，我们称之为“根”。 除了“根”之外，每栋房子有且只有一个“父“房子与之相连。一番侦察之后，聪明的小偷意识到“这个地方的所有房屋的排列类似于一棵二叉树”。 如果两个直接相连的房子在同一天晚上被打劫，房屋将自动报警。 计算在不触动警报的情况下，小偷一晚能够盗取的最高金额。 示例 1: 输入: [3,2,3,null,3,null,1] 3 / \\ 2 3 \\ \\ 3 1输出: 7解释: 小偷一晚能够盗取的最高金额 = 3 + 3 + 1 = 7.示例 2: 输入: [3,4,5,1,3,null,1] 3 / \\ 4 5 / \\ \\ 1 3 1 输出: 9解释: 小偷一晚能够盗取的最高金额 = 4 + 5 = 9 思路： 我们可以先不管这个否是二叉树,我们发现,如果我们选了 cur 这个节点 那么就说明 我们不能选它的所有子节点(还有父节点)。对于每一个节点，都只有选和不选两种情况。我们每次考虑一棵子树，那么根只有两种情况，选和不选(我们让dp[0]表示不选,dp[1]表示选)。对于选择了根,那么我们就不能选它的儿子了如果没有选根，我们就可以任意选了(即选最大的那一个) 12345678910111213141516171819202122# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def dp(self , cur : TreeNode) -&gt; List[int] : if not cur : return [0,0] l = self.dp(cur.left) r = self.dp(cur.right) return [max(l)+max(r),cur.val+l[0]+r[0]] def rob(self, root: TreeNode) -&gt; int: return max(self.dp(root))","link":"/2019/12/12/打家劫舍/"},{"title":"Gitlab跨版本升级","text":"最近leader让调研一下gitlab升级方案，测试了两次，都成功了，暂时没有发现网上有比较完整的升级博客或是文档，所以我总结了一下，后续有时间再补充一下升级过程中遇到的问题。 推荐升级路径：Gitlab-ce无法保证各大版本之间无缝升级，所以推荐升级路径： 公司的升级顺序按理应为： 123456789101112131415161718199.4.3 -&gt; 9.5.10 -&gt; 10.8.7 -&gt;^ ^ 11.11.8 -&gt;| | ^ ^ 12.5.5| | | | ^| | | | || | | | || | | | +| | | | Target version| | | | | | | | | | | +| | | Last release in the 11.x major version series| | +| | Last release in the 10.x major version series| +| Last release in the 9.x major version series+Current release to upgrade 可以看到我们已经下载好了到12.5.5的安装包： 帮助升级文档 备份文件：(可选) 1234567# rsync同步 保证目录权限rsync -av /var/opt/gitlab/backups /data/git/backups# 配置文件中添加如下配置 覆盖默认配置vim /etc/gitlab/gitlab.rb# gitlab_rails[&apos;backup_path&apos;] = &quot;/var/opt/gitlab/backups&quot; gitlab_rails[&apos;backup_path&apos;] = &quot;/data/git/backups&quot; Gitlab版本说明Gitlab采用 (Major).(Minor).(Patch)的命名方式： 例如，对于GitLab版本10.5.7： 10代表主要版本。主要版本是10.0.0，但通常称为10.0。 5代表次要版本。次要版本是10.5.0，但通常称为10.5。 7 代表补丁号。 特定版本的更改更新到主要版本可能需要一些手动干预。有关详细信息，请检查要更新的版本： GitLab 12 GitLab 11 GitLab 11具体的变化TLS v1.1弃用从GitLab 12.0开始，默认情况下将禁用TLS v1.1以提高安全性。 这减轻了许多问题，包括但不限于Heartbleed，并使GitLab开箱即用，符合PCI DSS 3.1标准。 详细了解我们的博客中不推荐使用TLS v1.1的原因。 客户支持TLS v1.2 Git-Credential-Manager - 自1.14.0以来的支持 红帽企业Linux 6上的git - 自6.8以来的支持 Red Hat Enteprirse Linux 7上的git - 自7.2以来的支持 JGit / Java - 自JDK 7以来的支持 Visual Studio - 自2017年版以来的支持 修改或添加gitlab.rb并gitlab-ctl reconfigure立即运行以禁用TLS v1.1： 1nginx[&apos;ssl_protocols&apos;] = &quot;TLSv1.2&quot; 升级先决条件要成功升级到GitLab 11.0，用户需要满足以下要求： 用户应该在10.x系列中运行最新版本。即10.8.7 现在已删除10.x系列中已弃用的配置（下面列出）。需要将其删除/etc/gitlab/gitlab.rb。然后运行gitlab-ctl reconfigure以应用配置更改。 如果不满足上述任一要求，升级过程将中止而不更改用户的现有安装。这是为了确保用户不会因为这些不受支持的配置而导致Gitlab损坏。 删除配置以下配置在10.x系列中已弃用，现已删除： 与Mattermost相关的配置 - 除了GitLab-Mattermost集成所需的基本配置外，已删除对大多数Mattermost相关配置的支持。查看官方文档了解详细信息 旧版git_data_dir配置，用于设置数据存储位置。它现在已经被git_data_dirs 配置所取代。查看官方文档了解详细信息 旧格式的git_data_dirs配置已被替换为新格式，允许更精细的颗粒控制。查看官方文档了解详细信息 次要版本中引入的更改11.2默认情况下禁用机架攻击。要继续使用Rack Attack，您必须手动启用它。 11.4 捆绑Redis的版本已升级到3.2.12。这是一个修复多个漏洞的关键安全更新。升级到11.4后，运行gitlab-ctl restart redis以确保加载新版本。 Prometheus的捆绑版本 已升级到2.4.2，默认情况下新安装将使用它。Prometheus版本2使用与版本1不兼容的数据格式。 对于希望保留Prometheus版本1数据的用户，提供了一个命令行工具来升级其Prometheus服务并将数据迁移到新Prometheus版本支持的格式。可以使用以下命令调用此工具： 1sudo gitlab-ctl prometheus-upgrade 此工具将现有数据转换为最新Prometheus版本支持的格式。根据数据量，此过程可能需要数小时。如果用户不想迁移数据，但是从干净的数据库开始，则可以将--skip-data-migrationflag 传递给上面的命令。 注意：Prometheus服务将在迁移过程中停止。 要了解其他支持的选项，--help请将flag 传递给上面的命令。 程序包升级期间不会自动调用此工具。用户必须手动运行才能迁移到最新版本的Prometheus，并建议尽快进行。因此，升级到11.4的现有用户将继续使用Prometheus 1.x，直到他们手动迁移到2.x版本。 对早期版本的GitLab附带的Prometheus 1.x版本的支持已被弃用，将在GitLab 12.0中完全删除。仍在使用这些版本的用户将在重新配置期间显示弃用警告。使用GitLab 12.0 Prometheus将自动升级到2.x，Prometheus 1.0数据将无法迁移。 11.6 如果在Redis HA模式下配置GitLab，默认情况下将禁用gitlab-monitor的Sidekiq探针。要手动启用它，用户可以gitlab_monitor['probe_sidekiq'] = true在/etc/gitlab/gitlab.rb文件中进行设置。但是，在Redis HA模式下手动启用时，用户应使用gitlab_rails['redis_*']设置将探针指向连接到实例的Redis实例。 有效的示例配置是： 1234gitlab_monitor[&apos;probe_sidekiq&apos;] = truegitlab_rails[&apos;redis_host&apos;] = &lt;IP of Redis master node&gt;gitlab_rails[&apos;redis_port&apos;] = &lt;Port where Redis runs in master node&gt;gitlab_rails[&apos;redis_password&apos;] = &lt;Password to connect to Redis master&gt; 注意：在上面的配置中，当主节点发生故障后发生故障转移时，gitlab-monitor仍将探测原始主节点，因为它是在中指定的gitlab.rb。用户必须手动更新gitlab.rb以将其指向新的主节点。 Ruby已更新至2.5.3。GitLab将在升级期间关闭，直到重新启动独角兽进程。重启在结束时自动完成gitlab-ctl reconfigure，默认情况下在升级时运行。 GitLab 10 GitLab 8 GitLab 7 GitLab 6 升级文档 操作系统 Ubuntu 16.04.1 LTS 系统配置 CPU：8核 内存：8G 磁盘：2T（/var/opt/gitlab） 当前版本 GitLab：9.4.3 GitLab Shell：5.3.1 GitLab Workhorse：v2.3.0 GitLab API：v4 Git：2.13.0 Ruby：2.3.3p222 Rails：4.2.8 postgresql：9.6.3 目标版本 GitLab：12.5.5 GitLab Shell：10.2.0 GitLab Workhorse：v8.14.1 GitLab API：v4 Git：2.7.4 Ruby：2.6.3p62 Rails：5.2.3 postgresql：10.9 升级步骤获取安装包 gitlab-ce_9.5.10-ce.0_amd64.deb gitlab-ce_10.8.7-ce.0_amd64.deb gitlab-ce_11.11.8-ce.0_amd64.deb gitlab-ce_12.5.5-ce.0_amd64.deb 说明：由于直接从9.xx升级到12.xx，中间数据库结构变化很大，9.4.3- &gt;9.5.10- &gt;10.8.7- &gt;11.11.8-&gt;12.5.5 目前版本： 停止对外提供服务，避免数据不一致。(每次升级都必须) 123gitlab-ctl stop unicorngitlab-ctl stop sidekiqgitlab-ctl stop nginx 进行gitlab全备份或rsync（见上） su do gitlab-rake gitlab:backup:create 升级到9.5.10版本 sudo dpkg -i gitlab-ce_9.5.10-ce.0_amd64.deb 重启一下：sudo gitlab-ctl restart 登录gitlab页面检查（该版本已经支持healthcheck接口，通过healthcheck接口检查服务是否正常） ![image-20191218174321793](/Users/yangjiacheng/Library/Application Support/typora-user-images/image-20191218174321793.png) 升级到10.8.7版本 sudo dpkg -i gitlab-ce_10.8.7-ce.0_amd64.deb 重启一下：sudo gitlab-ctl restart 登录gitlab页面检查（通过healthcheck接口检查服务是否正常） 升级到11.11.8版本 sudo dpkg -i gitlab-ce_10.8.7-ce.0_amd64.deb 重启一下：sudo gitlab-ctl restart 登录gitlab页面检查（通过healthcheck接口检查服务是否正常） 升级到12.5.5版本 sudo dpkg -i gitlab-ce_12.5.5-ce.0_amd64.deb 重启一下：sudo gitlab-ctl restart 登录gitlab页面检查（通过healthcheck接口检查服务是否正常） 至此gitlab升级完毕 以上过程已经在测试环境完成测试 回滚方案： 将备份的文件在备份机器上进行恢复 gitlab-rake gitlab:backup:restore force=yes BACKUP=版本号 一旦升级失败，可以将域名指向备份机器","link":"/2019/12/18/Gitlab跨版本升级/"},{"title":"系统设计题","text":"最近在看一些面试书籍，发现一些很好的看方法就搬运过来，侵删。以下内容来自《面试宝典》。 如何应对此类题目呢？在正式介绍基础知识之前，首先罗列几个常见的系统设计相关的面试笔试题，如下所示： （1）设计一个 DNS 的 Cache 结构，要求能够满足每秒 5000 次以上的查询，满足 IP 数据的快速插入，查询的速度要快（题目还给出了一系列的数据，比如站点数总共为 5000 万、IP 地址有 1000 万等）。 （2）有 N 台机器，M 个文件，文件可以以任意方式存放到任意机器上，文件可任意分割成若干块。假设这 N 台机器的宕机率小于 1/3，想在宕机时可以从其他未宕机的机器中完整导出这 M 个文件，求最好的存放与分割策略。 （3）假设有 30 台服务器，每台服务器上面都存有上百亿条数据（有可能重复），如何找出这 30 台机器中，根据某关键字，重复出现次数最多的前 100 条？要求使用 Hadoop 来实现。 （4）设计一个系统，要求写速度尽可能快，并说明设计原理。 （5）设计一个高并发系统，说明架构和关键技术要点。 （6）有 25TB 的日志（query-＞queryinfo），日志在不断地增长，设计一个方案，给出一个 query 能快速返回 queryinfo。 以上所有问题中凡是不涉及高并发的，基本可以采用 Google 的三个技术解决，即 GFS、MapReduce 和 Bigtable，这三个技术被称为「Google 三驾马车」，Google 只公开了论文而未开源代码，开源界对此非常有兴趣，仿照这三篇论文实现了一系列软件，如 Hadoop、HBase、HDFS 及 Cassandra 等。 在 Google 这些技术还未出现之前，企业界在设计大规模分布式系统时，采用的架构往往是 database+sharding+cache，现在很多公司（比如 taobao、weibo.com）仍采用这种架构。在这种架构中，仍有很多问题值得去探讨。如采用什么数据库，是 SQL 界的 MySQL 还是 NoSQL 界的 Redis/TFS，两者有何优劣？采用什么方式 sharding（数据分片），是水平分片还是垂直分片？据网上资料显示，weibo.com 和 taobao 图片存储中曾采用的架构是 Redis/MySQL/TFS+sharding+cache，该架构解释如下：前端 cache 是为了提高响应速度，后端数据库则用于数据永久存储，防止数据丢失，而 sharding 是为了在多台机器间分摊负载。最前端由大块大块的 cache 组成，要保证至少 99% 的访问数据落在 cache 中，这样可以保证用户访问速度，减少后端数据库的压力。此外，为了保证前端 cache 中的数据与后端数据库中的数据一致，需要有一个中间件异步更新（为什么使用异步？理由简单：同步代价太高。异步有缺点，如何弥补？）数据，这个有些人可能比较清楚，新浪有个开源软件叫 Memcachedb（整合了 Berkeley DB 和 Memcached），正是完成此功能。另外，为了分摊负载压力和海量数据，会将用户微博信息经过分片后存放到不同节点上（称为「Sharding」）。 这种架构优点非常明显：简单，在数据量和用户量较小的时候完全可以胜任。但缺点是扩展性和容错性太差，维护成本非常高，尤其是数据量和用户量暴增之后，系统不能通过简单地增加机器解决该问题。 鉴于此，新的架构应运而生。新的架构仍然采用 Google 公司的架构模式与设计思想，以下将分别就此内容进行分析。 GFS：这是一个可扩展的分布式文件系统，用于大型的、分布式的、对大量数据进行访问的应用。它运行于廉价的普通硬件上，提供容错功能。现在开源界有 HDFS（Hadoop Distributed File System），该文件系统虽然弥补了数据库 +sharding 的很多缺点，但自身仍存在一些问题，比如：由于采用 master/slave 架构，因此存在单点故障问题；元数据信息全部存放在 master 端的内存中，因而不适合存储小文件，或者说如果存储大量小文件，那么存储的总数据量不会太大。 MapReduce：这是针对分布式并行计算的一套编程模型。其最大的优点是：编程接口简单，自动备份（数据默认情况下会自动备三份），自动容错和隐藏跨机器间的通信。在 Hadoop 中，MapReduce 作为分布计算框架，HDFS 作为底层的分布式存储系统，但 MapReduce 不是与 HDFS 耦合在一起的，完全可以使用自己的分布式文件系统替换掉 HDFS。当前 MapReduce 有很多开源实现，如 Java 实现 Hadoop MapReduce，C++ 实现 Sector/sphere 等，甚至有些数据库厂商将 MapReduce 集成到数据库中了。 BigTable：俗称「大表」，是用来存储结构化数据的，作者觉得，BigTable 在开源界最火爆，其开源实现最多，包括 HBase、Cassandra 和 levelDB 等，使用也非常广泛。 除了 Google 的这「三驾马车」以外，还有其他一些技术可供学习与使用： Dynamo：亚马逊的 key-value 模式的存储平台，可用性和扩展性都很好，采用 DHT（DistributedHashTable）对数据分片，解决单点故障问题，在 Cassandra 中，也借鉴了该技术，在 BT 和电驴这两种下载引擎中，也采用了类似算法。 虚拟节点技术：该技术常用于分布式数据分片中。具体应用场景是：有一大块数据（可能 TB 级或者 PB 级），需按照某个字段（key）分片存储到几十（或者更多）台机器上，同时想尽量负载均衡且容易扩展。传统的做法是：Hash（key）mod N，这种方法最大的缺点是不容易扩展，即增加或者减少机器均会导致数据全部重分布，代价太大。于是新技术诞生了，其中一种是上面提到的 DHT，现在已经被很多大型系统采用，还有一种是对「Hash（key）mod N」的改进：假设要将数据分布到 20 台机器上，传统做法是 Hash（key）mod 20，而改进后，N 取值要远大于 20，比如是 20000000，然后采用额外一张表记录每个节点存储的 key 的模值，比如： node1：0～1000000 node2：1000001～2000000 …… 这样，当添加一个新的节点时，只需将每个节点上部分数据移动给新节点，同时修改一下该表即可。 Thrift：Thrift 是一个跨语言的 RPC 框架，分别解释「RPC」和「跨语言」如下：RPC 是远程过程调用，其使用方式与调用一个普通函数一样，但执行体发生在远程机器上；跨语言是指不同语言之间进行通信，比如 C/S 架构中，Server 端采用 C++ 编写，Client 端采用 PHP 编写，怎样让两者之间通信，Thrift 是一种很好的方式。 本篇最前面的几道题均可以映射到以上几个系统的某个模块中，如： （1）关于高并发系统设计，主要有以下几个关键技术点：缓存、索引、数据分片及锁粒度尽可能小。 （2）题目 2 涉及现在通用的分布式文件系统的副本存放策略。一般是将大文件切分成小的 block（如 64MB）后，以 block 为单位存放三份到不同的节点上，这三份数据的位置需根据网络拓扑结构配置，一般而言，如果不考虑跨数据中心，可以这样存放：两个副本存放在同一个机架的不同节点上，而另外一个副本存放在另一个机架上，这样从效率和可靠性上，都是最优的（这个 Google 公布的文档中有专门的证明，有兴趣的可参阅一下）。如果考虑跨数据中心，可将两份存在一个数据中心的不同机架上，另一份放到另一个数据中心。 （3）题目 4 涉及 BigTable 的模型。主要思想是将随机写转化为顺序写，进而大大提高写速度。具体是：由于磁盘物理结构的独特设计，其并发的随机写（主要是因为磁盘寻道时间长）非常慢，考虑到这一点，在 BigTable 模型中，首先会将并发写的大批数据放到一个内存表（称为「memtable」）中，当该表大到一定程度后，会顺序写到一个磁盘表（称为「SSTable」）中，这种写是顺序写，效率极高。此时可能有读者问，随机读可不可以这样优化？答案是：看情况。通常而言，如果读并发度不高，则不可以这么做，因为如果将多个读重新排列组合后再执行，系统的响应时间太慢，用户可能接受不了，而如果读并发度极高，也许可以采用类似机制。","link":"/2020/02/28/SystemDesign/"},{"title":"K8s小练习","text":"最近在知乎上发现了一个专栏，云计算技术前线,每天有一道题，对于检测基础来说很不错，由此我把它总结到这里。 第一题|Daemonset知识知识点初探以下 Daemonset yaml 中，哪些是正确的？（多选） A. apiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd-elasticsearch namespace: default labels: k8s-app: fluentd-logging spec: selector: matchLabels: name: fluentd-elasticsearch template: metadata: labels: name: fluentd-elasticsearch spec: containers: - name: fluentd-elasticsearch image: gcr.io/fluentd-elasticsearch/fluentd:v2.5.1 restartPolicy: Never B. apiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd-elasticsearch namespace: default labels: k8s-app: fluentd-logging spec: selector: matchLabels: name: fluentd-elasticsearch template: metadata: labels: name: fluentd-elasticsearch spec: containers: - name: fluentd-elasticsearch image: gcr.io/fluentd-elasticsearch/fluentd:v2.5.1 restartPolicy: Onfailure C. apiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd-elasticsearch namespace: default labels: k8s-app: fluentd-logging spec: selector: matchLabels: name: fluentd-elasticsearch template: metadata: labels: name: fluentd-elasticsearch spec: containers: - name: fluentd-elasticsearch image: gcr.io/fluentd-elasticsearch/fluentd:v2.5.1 restartPolicy: Always D. apiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd-elasticsearch namespace: default labels: k8s-app: fluentd-logging spec: selector: matchLabels: name: fluentd-elasticsearch template: metadata: labels: name: fluentd-elasticsearch spec: containers: - name: fluentd-elasticsearch image: gcr.io/fluentd-elasticsearch/fluentd:v2.5.1 解析：CD restartPolicy 字段，可选值为 Always、OnFailure 和 Never。默认为 Always。 一个Pod中可以有多个容器，restartPolicy适用于Pod 中的所有容器。restartPolicy作用是，让kubelet重启失败的容器。 另外Deployment、Statefulset的restartPolicy也必须为Always，保证pod异常退出，或者健康检查livenessProbe失败后由kubelet重启容器。https://kubernetes.io/zh/docs/concepts/workloads/controllers/deployment/ Job和CronJob是运行一次的pod，restartPolicy只能为OnFailure或Never，确保容器执行完成后不再重启。https://kubernetes.io/docs/conc 第二题| Daemonset、对接存储CSI知识点在Kubernetes PVC+PV体系下通过CSI实现的volume plugins动态创建pv到pv可被pod使用有哪些组件需要参与？ 123A. PersistentVolumeController + CSI-Provisoner + CSI controller pluginB. AttachDetachController + CSI-Attacher + CSI controller pluginC. Kubelet + CSI node plugin 解析：ABC k8s中，利用PVC 描述Pod 所希望使用的持久化存储的大小，可读写权限等，一般由开发人员去创建；利用PV描述具体存储类型，存储地址，挂载目录等，一般由运维人员去提前创建。而不是直接在pod里写上volume的信息。一来可以使得开发运维职责分明，二来利用PVC、PV机制，可以很好扩展支持市面上不同的存储实现，如k8s v1.10版本对Local Persistent Volume的支持。 我们试着理一下Pod创建到volume可用的整体流程。 用户提交请求创建pod，PersistentVolumeController发现这个pod声明使用了PVC，那就会帮它找一个PV配对。 没有现成的PV，就去找对应的StorageClass，帮它新创建一个PV，然后和PVC完成绑定。 新创建的PV，还只是一个API 对象，需要经过“两阶段处理”，才能变成宿主机上的“持久化 Volume”真正被使用： 第一阶段由运行在master上的AttachDetachController负责，为这个PV完成 Attach 操作，为宿主机挂载远程磁盘； 第二阶段是运行在每个节点上kubelet组件的内部，把第一步attach的远程磁盘 mount 到宿主机目录。这个控制循环叫VolumeManagerReconciler，运行在独立的Goroutine，不会阻塞kubelet主控制循环。 完成这两步，PV对应的“持久化 Volume”就准备好了，POD可以正常启动，将“持久化 Volume”挂载在容器内指定的路径。 k8s支持编写自己的存储插件FlexVolume 与 CSI。不管哪种方式，都需要经过“两阶段处理”，FlexVolume相比CSI局限性大，一般我们采用CSI方式对接存储。 CSI 插件体系的设计思想把这个Provision阶段（动态创建PV），以及 Kubernetes 里的一部分存储管理功能，从主干代码里剥离出来，做成了几个单独的组件。这些组件会通过 Watch API 监听 Kubernetes 里与存储相关的事件变化，比如 PVC 的创建，来执行具体的存储管理动作。 上图中CSI这套存储插件体系中三个独立的外部组件（External Components），即：Driver Registrar、External Provisioner 和 External Attacher，对应的是从 Kubernetes 项目里面剥离出来的部分存储管理功能。 我们需要实现Custom Components这一个二进制，会以gRpc方式提供三个服务：CSI Identity、CSI Controller、CSI Node。 Driver Registrar 组件，负责将插件注册到 kubelet 里面；Driver Registrar调用CSI Identity 服务来获取插件信息；External Provisioner 组件监听APIServer 里的 PVC 对象。当一个 PVC 被创建时，它就会调用 CSI Controller 的 CreateVolume 方法，创建对应 PV； External Attacher 组件负责Attach阶段。Mount阶段由kubelet里的VolumeManagerReconciler控制循环直接调用CSI Node服务完成。 两阶段完成后，kubelet将mount参数传递给docker，创建、启动容器。 整体流程如下图： 第三题|对接CSI存储知识通过单个命令创建一个deployment并暴露Service。deployment和Service名称为cka-1120，使用nginx镜像， deployment拥有2个pod 12345678910[root@peter ~]# kubectl run cka-1120 --replicas 2 --expose=true --port=80 --image=nginxkubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.service/cka-1120 createddeployment.apps/cka-1120 created[root@peter ~]# kubectl get all | grep cka-1120pod/cka-1120-554b9c4798-7jcrb 1/1 Running 0 118mpod/cka-1120-554b9c4798-fpjwj 1/1 Running 0 118mservice/cka-1120 ClusterIP 10.108.140.25 &lt;none&gt; 80/TCP 118mdeployment.apps/cka-1120 2/2 2 2 118m 解析： 官网中提供了详细的kubectl使用方法，位于REFERENCE–&gt;&gt;kubectl CLI–&gt;&gt;kubectl Commands标签下。即：https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#runkubectl run会创建deployment或者job来管理Pod，命令语法如下： 1kubectl run NAME --image=image [--env=&quot;key=value&quot;] [--port=port] [--replicas=replicas] [--dry-run=bool] [--overrides=inline-json] [--command] -- [COMMAND] [args...] NAME指定deployment和service的名称；–replicas缩写-r，指定实例数，默认为1；–expose如果为true，会创建有ClusterIP的service，默认为false；–port表示容器暴露的端口，如果expose为true，该端口也是service的端口；–image指定容器用的镜像；–dry-run为true时，只打印将要发送的对象，而不真正发送它，默认为false。 创建名为cka-1120-01，带环境变量的deployment 1kubectl run cka-1120-01 --image=nginx --env=&quot;DNS_DOMAIN=cluster.local&quot; --env=&quot;POD_NAMESPACE=default&quot; 创建名为cka-1120-02，带label的deployment 1kubectl run cka-1120-02 --image=nginx --labels=&quot;app=nginx,env=prod&quot; 还有一个–restart参数，默认为Always，如果设置为OnFailure，则job会被创建；如果设置为Never，则普通Pod会被创建。 123456[root@peter ~]# kubectl run cka-1120-03 --image=nginx --restart=OnFailurekubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.job.batch/cka-1120-03 created[root@peter ~]# [root@peter ~]# kubectl run cka-1120-04 --image=nginx --restart=Neverpod/cka-1120-04 created 参数–schedule指定cronjob的定时规则，如果指定该参数，则会创建出cronjob 12[root@peter ~]# kubectl run pi --schedule=&quot;0/5 * * * ?&quot; --image=perl --restart=OnFailure -- perl -Mbignum=bpi -wle &apos;print bpi(2000)&apos;cronjob.batch/pi created 目前不支持直接创建Statefulset、Daemonset等资源对象 kubectl run执行后，到底发生了什么？有必要看看kubectl源码，入口函数在$GOPATHsrck8s.iokubernetescmdclicheckcheckcliconventions.go中 其中cmd.NewKubectlCommand为构建kubectl以及其子命令行参数。最终的执行业务逻辑的代码都在pkgkubectl包下面。不同的子命令：apply、run、create入口对应的在pkgkubectlcmd下面： 最重要的o.Run(f, cmd, args)中会对kubectl run传入的参数进行一系列校验，填充默认值。 在360行调用o.createGeneratedObject根据不同的generator生成deployment、cronjob、job、pod等资源对象，并向apiserver发送创建请求。 如果设置了expose为true，在372行，同样的调用o.createGeneratedObject生成并创建service。 o.createGeneratedObject方法第649行，根据不同的generator实现生成不同的资源对象。 run命令对应的generator实现有以下几种，代码位于pkgkubectlgenerateversionedgenerator.go中的DefaultGenerators函数。 1234567891011case &quot;run&quot;: generator = map[string]generate.Generator{ RunV1GeneratorName: BasicReplicationController{}, RunPodV1GeneratorName: BasicPod{}, DeploymentV1Beta1GeneratorName: DeploymentV1Beta1{}, DeploymentAppsV1Beta1GeneratorName: DeploymentAppsV1Beta1{}, DeploymentAppsV1GeneratorName: DeploymentAppsV1{}, JobV1GeneratorName: JobV1{}, CronJobV2Alpha1GeneratorName: CronJobV2Alpha1{}, CronJobV1Beta1GeneratorName: CronJobV1Beta1{}, } o.createGeneratedObject方法第689行对生成的资源对象向APIServer发送http创建请求。 具体的kubectl run命令的代码，感兴趣的同学可以进一步深挖，我也会在后续的源码分析系列文章中进行更详细的解析。 第四题|熟练掌握kubectl命令进行创建资源对象操作通过命令行，使用nginx镜像创建一个pod并手动调度到节点名为node1121节点上，Pod的名称为cka-1121，答题最好附上，所用命令、创建Pod所需最精简的yaml；如果评论有限制，请把注意点列出，主要需列出手动调度怎么做？ 注意：手动调度是指不需要经过kube-scheduler去调度。 123456789101112apiVersion: v1kind: Podmetadata: name: cka-1121 labels: app: cka-1121spec: containers: - name: cka-1121 image: busybox command: ['sh', '-c', 'echo Hello CKA! &amp;&amp; sleep 3600'] nodeName: node1121 解析： 官网中调度器地址：https://kubernetes.io/docs/concepts/scheduling/kube-scheduler/调度器命令行参数：https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/ 调度器kube-scheduler分为预选、优选、pod优先级抢占、bind阶段； 预选：从podQueue的待调度队列中弹出需要调度的pod，先进入预选阶段，预选函数来判断每个节点是否适合被该Pod调度。 优选：从预选筛选出的满足的节点中选择出最优的节点。 pod优先级抢占：如果预选和优选调度失败，则会尝试将优先级低的pod剔除，让优先级高的pod调度成功。 bind：上述步骤完成后，调度器会更新本地缓存，但最后需要将绑定结果提交到etcd中，需要调用Apiserver的Bind接口完成。 以下k8s源码版本为1.13.2 我们去查看kube-scheduler源码，调度器通过list-watch机制，监听集群内Pod的新增、更新、删除事件，调用回调函数。指定nodeName后将不会放入到未调度的podQueue队列中，也就不会走上面这几个阶段。具体可以来到pkgschedulerfactoryfactory.go源码中的NewConfigFactory函数中： 其中在构建pod资源对象新增、更新、删除的回调函数时，分已被调度的和未被调度的回调。 已被调度的回调：已被调度的pod根据FilterFunc中定义的逻辑过滤，nodeName不为空，返回true时，将会走Handler中定义的AddFunc、UpdateFunc、DeleteFunc，这个其实最终不会加入到podQueue中，但需要加入到本地缓存中，因为调度器会维护一份节点上pod列表的缓存。 1234567891011121314151617181920212223242526// scheduled pod cache 已被调度的 args.PodInformer.Informer().AddEventHandler( cache.FilteringResourceEventHandler{ FilterFunc: func(obj interface{}) bool { switch t := obj.(type) { case *v1.Pod: //nodeName不为空,返回true;且返回true时将被走AddFunc、UpdateFunc、DeleteFunc,这个其实最终不会加入到podQueue中 return assignedPod(t) case cache.DeletedFinalStateUnknown: if pod, ok := t.Obj.(*v1.Pod); ok { return assignedPod(pod) } runtime.HandleError(fmt.Errorf(\"unable to convert object %T to *v1.Pod in %T\", obj, c)) return false default: runtime.HandleError(fmt.Errorf(\"unable to handle object in %T: %T\", c, obj)) return false } }, Handler: cache.ResourceEventHandlerFuncs{ AddFunc: c.addPodToCache, UpdateFunc: c.updatePodInCache, DeleteFunc: c.deletePodFromCache, }, }, ) 未被调度的回调：未被调度的pod根据FilterFunc中定义的逻辑过滤，nodeName为空且pod的SchedulerName和该调度器的名称一致时返回true；返回true时，将会走Handler中定义的AddFunc、UpdateFunc、DeleteFunc，这个最终会加入到podQueue中。 1234567891011121314151617181920212223242526// unscheduled pod queue 没有被调度的args.PodInformer.Informer().AddEventHandler( cache.FilteringResourceEventHandler{ FilterFunc: func(obj interface{}) bool { switch t := obj.(type) { case *v1.Pod: //nodeName为空且pod的SchedulerName和该调度器的名称一致时返回true;且返回true时将被加入到pod queue return !assignedPod(t) &amp;&amp; responsibleForPod(t, args.SchedulerName) case cache.DeletedFinalStateUnknown: if pod, ok := t.Obj.(*v1.Pod); ok { return !assignedPod(pod) &amp;&amp; responsibleForPod(pod, args.SchedulerName) } runtime.HandleError(fmt.Errorf(\"unable to convert object %T to *v1.Pod in %T\", obj, c)) return false default: runtime.HandleError(fmt.Errorf(\"unable to handle object in %T: %T\", c, obj)) return false } }, Handler: cache.ResourceEventHandlerFuncs{ AddFunc: c.addPodToSchedulingQueue, UpdateFunc: c.updatePodInSchedulingQueue, DeleteFunc: c.deletePodFromSchedulingQueue, }, },) 手动调度适用场景： 调度器不工作时，可设置nodeName临时救急 ； 可以封装成自己的调度器； 扩展点： 过去几个版本的Daemonset都是由controller直接指定pod的运行节点，不经过调度器。 直到1.11版本，DaemonSet的pod由scheduler调度才作为alpha特性引入 static Pod，这种其实也属于节点固定，但这种Pod局限很大，比如：不能挂载configmaps和secrets等，这个由Admission Controllers控制。 下面简单说一下静态Pod： 静态Pod官网说明：https://kubernetes.io/docs/tasks/configure-pod-container/static-pod/ 静态 pod指在特定的节点上直接通过 kubelet守护进程进行管理，APIServer无法管理。它没有跟任何的控制器进行关联，kubelet 守护进程对它进行监控，如果崩溃了，kubelet 守护进程会重启它。Kubelet 通过APIServer为每个静态 pod 创建 镜像 pod，这些镜像 pod 对于 APIServer是可见的（即kubectl可以查询到这些Pod），但是不受APIServer控制。 具体static pod yaml文件放到哪里，需要在kubelet配置中指定，先找到kubelet配置文件： 1systemctl status kubelet 找到config.yaml文件，里面指定了staticPodPath，kubeadm安装的集群，master节点上的kube-apiserver、kube-scheduler、kube-controller-manager、etcd就是通过static Pod方式部署的。 第五题|Deployment通过命令行，创建两个deployment。 需要集群中有2个节点； 第1个deployment名称为cka-1122-01，使用nginx镜像，有2个pod，并配置该deployment自身的pod之间在节点级别反亲和； 第2个deployment名称为cka-1122-02，使用nginx镜像，有2个pod，并配置该deployment的pod与第1个deployment的pod在节点级别亲和； 第一个deployment：cka-1122-01 1234567891011121314151617181920212223242526272829apiVersion: apps/v1kind: Deploymentmetadata: labels: app: cka-1122-01 name: cka-1122-01spec: replicas: 2 selector: matchLabels: app: cka-1122-01 template: metadata: labels: app: cka-1122-01 spec: containers: - image: nginx name: cka-1122-01 affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: labelSelector: matchExpressions: - key: app operator: In values: - cka-1122-01 topologyKey: \"kubernetes.io/hostname\" 第二个deployment：cka-1122-02 1234567891011121314151617181920212223242526272829apiVersion: apps/v1kind: Deploymentmetadata: labels: app: cka-1122-02 name: cka-1122-02spec: replicas: 2 selector: matchLabels: app: cka-1122-02 template: metadata: labels: app: cka-1122-02 spec: containers: - image: nginx name: cka-1122-02 affinity: podAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - cka-1122-01 topologyKey: \"kubernetes.io/hostname\" 最终调度结果： 12345NAME READY STATUS RESTARTS AGE IP NODEcka-1122-01-5df9bdf8c9-qwd2v 1/1 Running 0 8m 10.192.4.2 node-1cka-1122-01-5df9bdf8c9-r4rhs 1/1 Running 0 8m 10.192.4.3 node-2 cka-1122-02-749cd4b846-bjhzq 1/1 Running 0 10m 10.192.4.4 node-1cka-1122-02-749cd4b846-rkgpo 1/1 Running 0 10m 10.192.4.5 node-2 解析： 考点：k8s中的高级调度及用法。亲和性和反亲和性调度官方文档：https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ 将 Pod 调度到特定的 Node 上：nodeSelectornodeSelector是节点选择约束的最简单推荐形式。 nodeSelector是PodSpec下的一个字段。它指定键值对的映射。为了使Pod可以在节点上运行，该节点必须具有每个指定的键值对作为label。 语法格式：map[string]string 作用： – 匹配node.labels – 排除不包含nodeSelector中指定label的所有node – 匹配机制 —— 完全匹配 nodeSelector 升级版：nodeAffinity节点亲和性在概念上类似于nodeSelector，它可以根据节点上的标签来限制Pod可以被调度在哪些节点上。 红色框为硬性过滤：排除不具备指定label的node；在预选阶段起作用； 绿色框为软性评分：不具备指定label的node打低分， 降低node被选中的几率；在优选阶段起作用； 与nodeSelector关键差异 – 引入运算符：In，NotIn （labelselector语法） – 支持枚举label可能的取值，如 zone in [az1, az2, az3…] – 支持硬性过滤和软性评分 – 硬性过滤规则支持指定多条件之间的逻辑或运算 – 软性评分规则支持 设置条件权重值 让某些 Pod 分布在同一组 Node 上：podAffinityPod亲和性和反亲和性可以基于已经在节点上运行的Pod上的标签而不是基于节点上的标签，来限制Pod调度的节点。 规则的格式为： 如果该X已经在运行一个或多个满足规则Y的Pod，则该Pod应该（或者在反亲和性的情况下不应该）在X中运行。 Y表示为LabelSelector。X是一个拓扑域，例如节点，机架，云提供者区域，云提供者区域等。 红框硬性过滤： 排除不具备指定pod的node组；在预选阶段起作用； 绿框软性评分： 不具备指定pod的node组打低分， 降低该组node被选中的几率；在优选阶段起作用； 与nodeAffinity的关键差异 – 定义在PodSpec中，亲和与反亲和规则具有对称性 – labelSelector的匹配对象为Pod – 对node分组，依据label-key=topologyKey，每个labelvalue取值为一组 – 硬性过滤规则，条件间只有逻辑与运算 避免某些 Pod 分布在同一组 Node 上：podAntiAffinity 与podAffinity的差异 – 匹配过程相同 – 最终处理调度结果时取反 即 – podAffinity中可调度节点，在podAntiAffinity中为不可调度 – podAffinity中高分节点，在podAntiAffinity中为低分 第六题|deployment的升级回滚、滚动更新策略、roll、set image命令通过命令行，创建1个deployment，副本数为3，镜像为nginx:latest。然后滚动升 级到nginx:1.9.1，再回滚到原来的版本 要求：Deployment的名称为cka-1125，贴出用到的相关命令。 先创建deployment，可以用命令创建： 1kubectl run cka-1125 --image=nginx --replicas=3 也可以用以下yaml：cka-1125.yaml创建 12345678910111213141516171819apiVersion: apps/v1kind: Deploymentmetadata: labels: app: cka-1125 name: cka-1125spec: replicas: 3 selector: matchLabels: app: cka-1125 template: metadata: labels: app: cka-1125 spec: containers: - image: nginx name: cka-1125 创建： 1kubectl apply -f cka-1125.yaml 升级： 12kubectl set image deploy/cka-1125 cka-1125=nginx:1.9.1 --recorddeployment.extensions/cka-1125 image updated 回滚： 1234# 回滚到上一个版本kubectl rollout undo deploy/cka-1125# 回滚到指定版本kubectl rollout undo deploy/cka-1125 --to-revision=2 解析： 官方中set image命令：https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#set set image命令set image命令格式如下： 1kubectl set image (-f FILENAME | TYPE NAME) CONTAINER_NAME_1=CONTAINER_IMAGE_1 ... CONTAINER_NAME_N=CONTAINER_IMAGE_N [--record] –record指定，在annotation中记录当前的kubectl命令。 如果设置为false，则不记录命令。 如果设置为true，则记录命令。 默认为false。 12345678[root@peter test]# kubectl set image deploy/cka-1125 cka-1125=nginx:1.9.1 --recorddeployment.extensions/cka-1125 image updated[root@peter test]# [root@peter test]# kubectl rollout history deploy/cka-1125 deployment.extensions/cka-1125 REVISION CHANGE-CAUSE3 &lt;none&gt;4 kubectl set image deploy/cka-1125 cka-1125=nginx:1.9.1 --record=true 像上面这样，CHANGE-CAUSE中会有升级命令。 set image命令可以对：pod (po), replicationcontroller (rc), deployment (deploy), daemonset (ds), replicaset (rs)，statefulset(sts)进行操作。 roll命令roll命令官方文档：https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#rollout 可以管理deployments、daemonsets、statefulsets资源的回滚： 查询升级历史： 12345[root@peter test]# kubectl rollout history deploy/cka-1125 deployment.extensions/cka-1125 REVISION CHANGE-CAUSE1 &lt;none&gt;2 &lt;none&gt; 查看指定版本的详细信息： 1kubectl rollout history deploy/cka-1125 --revision=3 -o=yaml 回滚到上一个版本： 12[root@peter test]# kubectl rollout undo deploy/cka-1125 deployment.extensions/cka-1125 rolled back 或者回滚到指定版本： 12[root@peter test]# kubectl rollout undo deploy/cka-1125 --to-revision=3deployment.extensions/cka-1125 rolled back 其他roll子命令restart：资源将重新启动；status：展示回滚状态；resume：恢复被暂停的资源。控制器不会控制被暂停的资源。通过恢复资源，可以让控制器再次控制。 resume仅对deployment支持。pause：控制器不会控制被暂停的资源。 使用kubectl rollout resume来恢复暂停的资源。 当前，只有deployment支持被暂停。 滚动更新策略滚动更新官网文档：https://kubernetes.io/docs/concepts/workloads/controllers/deployment/ 123456minReadySeconds: 5strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 1 minReadySecondsKubernetes在等待设置的时间后才进行升级如果没有设置该值，Kubernetes会假设该容器启动起来后就提供服务了如果没有设置该值，在某些极端情况下可能会造成服务服务正常运行 maxSurge 控制滚动更新过程中副本总数超过DESIRED的上限。maxSurge可以是具体的整数，也可以是百分比，向上取整。maxSurge默认值为25%。 例如DESIRED为10，那么副本总数的最大值为roundUp(10 + 10*25%)=13,所以CURRENT为13。 maxUnavaible控制滚动更新过程中，不可用副本占DESIRED的最大比例。maxUnavailable可以是具体的整数，也可以是百分之百，向下取整。默认值为25%。 例如DESIRED为10，那么可用的副本数至少要为 10-roundDown(10*25%)=8所以AVAILABLE为8。 maxSurge越大，初始创建的新副本数量就越多；maxUnavailable越大，初始销毁的旧副本数目就越多。 第8题 | initContainer概念、用法、使用场景简介提供一个pod的yaml，要求添加Init Container，Init Container的作用是创建一个空文件，pod的Containers判断文件是否存在，不存在则退出 123456789101112131415161718192021222324apiVersion: v1kind: Podmetadata: labels: run: cka-1126 name: cka-1126spec: initContainers: - image: busybox name: init-c command: ['sh', '-c', 'touch /tmp/cka-1126'] volumeMounts: - name: workdir mountPath: \"/tmp\" containers: - image: busybox name: cka-1126 command: ['sh', '-c', 'ls /tmp/cka-1126 &amp;&amp; sleep 3600 || exit 1'] volumeMounts: - name: workdir mountPath: \"/tmp\" volumes: - name: workdir emptyDir: {} 主Container的command就是判断文件是否存在，存在则不退出，不存在则退出；也可以用以下if判断： 1command: ['sh', '-c', 'if [ -e /tmp/cka-1126 ];then echo \"file exits\";else echo \"file not exits\" &amp;&amp; exit 1;fi'] 本题的关键点是init容器与主容器需要共同挂载一个名为workdir的目录，init容器在里面创建一个空文件，主容器去检验文件是否存在，检验主要用的是shell的语法； 1command: [&apos;sh&apos;, &apos;-c&apos;, &apos;ls /tmp/cka-1126 &amp;&amp; sleep 3600 || exit 1&apos;] 解析： 这句话意思是：如果ls /tmp/cka-1126返回码为0，即文件存在，将sleep 3600秒；否则exit 1退出； 也可以用shell的if语法判断。 官方文档地址：https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-initialization/https://kubernetes.io/docs/concepts/workloads/pods/init-containers/ 梳理概念初始化容器，顾名思义容器启动的时候，会先启动可一个或多个容器，如果有多个，那么这几个Init Container按照定义的顺序依次执行，一个执行成功，才能执行下一个，只有所有的Init Container执行完后，主容器才会启动。由于一个Pod里的存储卷是共享的，所以Init Container里产生的数据可以被主容器使用到。 Init Container可以在多种K8S资源里被使用到如Deployment、Daemon Set、StatefulSet、Job等，但归根结底都是在Pod启动时，在主容器启动前执行，做初始化工作。 Init 容器支持应用容器的全部字段和特性，包括资源限制、数据卷和安全设置。然而，Init 容器不支持 Readiness Probe，因为它们必须在 Pod 就绪之前运行完成；在资源限制、调度方面也会略有不同。 应用场景等待其它模块Ready：比如有一个应用里面有两个容器化的服务，一个是Web Server，另一个是数据库。其中Web Server需要访问数据库。但是当我们启动这个应用的时候，并不能保证数据库服务先启动起来，所以可能出现在一段时间内Web Server连接数据库错误。为了解决这个问题，我们可以在运行Web Server服务的Pod里使用一个InitContainer，去检查数据库是否准备好，直到数据库可以连接，Init Container才结束退出，然后Web Server容器被启动，发起正式的数据库连接请求。 初始化配置：比如集群里检测所有已经存在的成员节点，为主容器准备好集群的配置信息，这样主容器起来后就能用这个配置信息加入集群；目前在容器化，初始化集群配置文件时经常用到； 提供一种阻塞容器启动的方式：必须在initContainer容器启动成功后，才会运行下一个容器，保证了一组条件运行成功的方式； 其它使用场景：将pod注册到一个中央数据库、下载应用依赖等。 Kubernetes 1.5 版本 开始支持在annotations下用http://pod.beta.kubernetes.io/init-containers申明initContainer，像以下这样。 123456789101112131415161718192021222324apiVersion: v1kind: Podmetadata: name: myapp-pod labels: app: myapp annotations: pod.beta.kubernetes.io/init-containers: '[ { \"name\": \"init-myservice\", \"image\": \"busybox\", \"command\": [\"sh\", \"-c\", \"until nslookup myservice; do echo waiting for myservice; sleep 2; done;\"] }, { \"name\": \"init-mydb\", \"image\": \"busybox\", \"command\": [\"sh\", \"-c\", \"until nslookup mydb; do echo waiting for mydb; sleep 2; done;\"] } ]'spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', 'echo The app is running! &amp;&amp; sleep 3600'] Kubernetes 1.6 版本的新语法将 Init 容器的声明移到 spec 下，但是老的 annotation 语法仍然可以使用。","link":"/2019/11/28/K8s小练习/"}],"tags":[{"name":"gerrit权限","slug":"gerrit权限","link":"/tags/gerrit权限/"},{"name":"MySQL","slug":"MySQL","link":"/tags/MySQL/"},{"name":"RAID","slug":"RAID","link":"/tags/RAID/"},{"name":"java","slug":"java","link":"/tags/java/"},{"name":"java关键字","slug":"java关键字","link":"/tags/java关键字/"},{"name":"test","slug":"test","link":"/tags/test/"},{"name":"Gitlab","slug":"Gitlab","link":"/tags/Gitlab/"},{"name":"Gerrit","slug":"Gerrit","link":"/tags/Gerrit/"},{"name":"面试","slug":"面试","link":"/tags/面试/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"LXC","slug":"LXC","link":"/tags/LXC/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"内核态","slug":"内核态","link":"/tags/内核态/"},{"name":"用户态","slug":"用户态","link":"/tags/用户态/"},{"name":"leetcode","slug":"leetcode","link":"/tags/leetcode/"},{"name":"gitlab update","slug":"gitlab-update","link":"/tags/gitlab-update/"},{"name":"k8s基础","slug":"k8s基础","link":"/tags/k8s基础/"}],"categories":[{"name":"Gerrit","slug":"Gerrit","link":"/categories/Gerrit/"},{"name":"MySQL","slug":"MySQL","link":"/categories/MySQL/"},{"name":"Linux","slug":"Linux","link":"/categories/Linux/"},{"name":"java基础","slug":"java基础","link":"/categories/java基础/"},{"name":"test","slug":"test","link":"/categories/test/"},{"name":"Gitlab","slug":"Gitlab","link":"/categories/Gitlab/"},{"name":"Python原理","slug":"Python原理","link":"/categories/Python原理/"},{"name":"Docker","slug":"Docker","link":"/categories/Docker/"},{"name":"Linux基础","slug":"Linux基础","link":"/categories/Linux基础/"},{"name":"算法","slug":"算法","link":"/categories/算法/"},{"name":"系统设计","slug":"系统设计","link":"/categories/系统设计/"},{"name":"k8s","slug":"k8s","link":"/categories/k8s/"}]}